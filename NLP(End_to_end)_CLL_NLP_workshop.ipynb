{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shakirakashif/jupyter-exploration/blob/main/NLP(End_to_end)_CLL_NLP_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'glove-global-vectors-for-word-representation:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1835%2F3176%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241006%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241006T220128Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8c1e7541e03158329cfad056b2955c2fb4012eb28d6a66bb3cec510f8d52f1f475a845c58ca76cc88748cba26ad0590c9a25a3f4a6bec7f6a3f1165a3fcb8316ca07a42b66ded180ff4d0c7edee8c8ce466b1b0576df2cca36b8d1996d2926dfb244ae1fc3d3fc0138543ca90911cd4053796751987703a1ae51b2d3151d8d8c8ce0d0c0bd828547a873cd961c7ce677a8a56dcdbed3506fde86565ac5589c3d765f264939b09bba12f19851b5f4eedf8266fb21b79dd0ca7c36c3fc7a8b122abca3bfd59d26454cb3184d6bab6d89afd6b387a755d4fafad72206ee4c8a9e9e1e1a470e696e766a05b0c3823e4e5393e30ef0e547985178cabe5a98f467147d,googlenewsvectorsnegative300:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F6763%2F9801%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241006%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241006T220128Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D55ee5810730b5b79f10e23db78de5ab8e50d7f4927ffa0379101ecdd87478c1972128c3b476e93f08c38a942391b1f9978c56c97d091accd13e506d458445044f91d29ceb089100d58d85a6a2c3bb11ab6070049b75a657067e4e086f21804b5f630e53a8e5408af54b86f3743f1a79b0ceae2a3400035794396cef4a3751fb3d3fb61e285a3d81fe768d4c9f40c9c4177248f5109bee4ca400bc15bb24bf4a1ea6f2b653a41789082329bb88415baa77947e76d2b2f56667addbc1d5187d3b186d1d65502ecdac3c2db7a01149f2cef35b2eb9195705763d298db2b75d4a3ced2877f089096a003f6355e9004cad82af889a2981f505ac68957f439be985445,fasttext-crawl-300d-2m:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F14154%2F19053%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241006%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241006T220128Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6d98c9782696e9fb9c0d8fdc9545c29a0fae5d5e87e10f1adedf2b619cbe73665c06ddebcefcbaba6d8b4c0798a124497f4bd1669bfd20d42c5855a6e08733a73ec7c80526af97b14a40d95fded6ec5c41be3cc331bbd497d91c307dc1a8d935a3afccc1b310685899ac65c647261844b42d7c88be3ddb94522035e46a3d63170bebe7850707aa5707e0ccf6b6501e1c73585ea338ebf2816a86b72d1c8463c6bd0921120d934ab1f15f38f3a58117315f1059d96f8fdc3f7f44852e018fc4b2b6c44f5364c4ab67c3b9cf9485502a49dd9ea03026d6a89df8d2ed09ada2606cd3e55654394aca4b70b8aed59070bbacd4afb36ce5e0f0189243b30db5ce9f02,imdb-dataset-of-50k-movie-reviews:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F134715%2F320111%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241006%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241006T220128Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9a1b1a3b3a9698895262d865be2532bd32ae9f802d03f5dc414a47aa18144a42bd185ff282c64c2e2b5386881ad4156dc282d9c7b14a9ef53d6841d62344e0a13fad7864134aa0f6444133aa0af755ff46c5aea1b009319590052f072c8db036f473ecf994eda88d977110e99e22c110cee3a3dedd9e673fedde03b3ec857ac341c573b6265d8c62b7a14eab2f989f1f5b969c8f9f18893aa0a916bdae6163230760b50b1a3047b0b149f0a72f068479f8c92117c96c5826375911c52a6556590a6ba4c78938739e9b3e95b94ebdf45ddb8a02c4bd10a0501cfb8b4ca26db248ae5f3d98cf97b3e2c6db2cd1203121f42e9688a31dd1d168e9d7ccbec63fcf6f,wikinews300d1msubwordvec:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F674789%2F1186714%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241006%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241006T220128Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3e8d726e17f42e835e9688eff0820e5ce86073243d16701d5a824e7edb710df7e40eac4da426dfe96c1e736ea5e55122ada9161023024af4118e4249ccf36d1fe6357c220ba5035604de2f754cf0bc6816d3fcea5ea2409411978eaf676acec388b74b59d7439b68695c13e3ac63552067fa2b906f2c2d854e0c90613d118071bb52646fc9b5ff9c01c7bdba8656fe03f2ed4d347b866ea93741b3489ac1964e662df27e0265bda77e71e1c7a943d4e360582d42855b7bcd7f730813402d4912aed9eb54010baa74abb8d238f4dde402113f52c9ac5bcddf0fb00320f9c23d6ce464f6738b684cf46ec419ba7d19d5054e956334b0adf36c4cbd8d65685017f5'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQiDzEQJ2iCy",
        "outputId": "3e67fe4b-18d7-4bb8-cd3e-6ad4415df2e6"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading glove-global-vectors-for-word-representation, 480172569 bytes compressed\n",
            "[==================================================] 480172569 bytes downloaded\n",
            "Downloaded and uncompressed: glove-global-vectors-for-word-representation\n",
            "Downloading googlenewsvectorsnegative300, 3408474711 bytes compressed\n",
            "[==================================================] 3408474711 bytes downloaded\n",
            "Downloaded and uncompressed: googlenewsvectorsnegative300\n",
            "Downloading fasttext-crawl-300d-2m, 1545551987 bytes compressed\n",
            "[=============================                     ] 899072000 bytes downloaded"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "aF2KAvj_2iC4"
      },
      "cell_type": "markdown",
      "source": [
        "## NLP Workshop:Dataset Preparation\n",
        "\n",
        "\n",
        "Authored by [abhilash1910](https://www.kaggle.com/abhilash1910)\n",
        "\n",
        "### Movie Reviews!!\n",
        "\n",
        "This is the first step in the learning curriculum where we will be exploring strategies and initial benchmark analysis for our dataset and derive important information from it. In this context, we will be focussing on preparing the dataset, eliminating the redundancies such as punctuations ,stopwords and estimating the meaningfulness of the data.\n",
        "\n",
        "The steps to be followed can be proposed:\n",
        "\n",
        "- Loading the dataset\n",
        "- Preprocessing of data\n",
        "    - Lemmatizing\n",
        "    - Tokenizing\n",
        "- Cleaning\n",
        "    - Stopword\n",
        "    - punctuations\n",
        "    - Common words\n",
        "    - URLs\n",
        "    - HTML tags\n",
        "    - Emojis\n",
        "    - Expanding abbreviations\n",
        "\n",
        "These steps are fundamental for any pipeline related to NLP , and this produces the transformed data which is suitable for analysis. This data without redundancies is fit for passing to any deep learning model  or statistical model or can be finetuned with more transformations. This would rather input more \"semantic\" sense to the data at hand.\n",
        "\n",
        "\n",
        "\n",
        "Lets get started!!\n",
        "\n",
        "<img src=\"https://www.denofgeek.com/wp-content/uploads/2019/07/endgame-recap-scaled.jpeg?fit=2560%2C1440\">"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "gvZ_y-BS2iC6"
      },
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UoqwubJi2iC7"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the Dataset\n",
        "\n",
        "This is the primary step of the entire pipeline. In this case, we have to load the dataset using pandas.\n",
        "In this case, we will be exploring different datasets for our use case. We will be using the [IMDB Movie Reviews Dataset](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) primarily for the initial use case.\n",
        "\n",
        "The data is collated from [Stanford Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) and the sentiment of the text corpus is either positive or negative.\n",
        "\n",
        "We will be analysing the data first in terms of the columns which it has."
      ]
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "z6vBhImi2iC8"
      },
      "cell_type": "code",
      "source": [
        "train_df=pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fkZYYJ9b2iC9"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Columns\n",
        "\n",
        "The \"review\" column contains the textual information(input features) and the \"sentiment\" column contains the output labels. The task of any classifier is to correctly predict the \"sentiment\" given any \"review\" or textual column. Hence we have to apply our data cleaning, transformation steps to the \"review\" column."
      ]
    },
    {
      "metadata": {
        "id": "oLFDECsp2iC-"
      },
      "cell_type": "markdown",
      "source": [
        "## Validating the number of entries\n",
        "\n",
        "This includes the number of entries we have in the dataset. Also we can have an analysis on the statistical aspects of the data which we will be exploring further through graphs and charts."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HhZXNWwT2iC-"
      },
      "cell_type": "code",
      "source": [
        "len(train_df),train_df.index.shape[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aXdURU8u2iC_"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing libraries\n",
        "\n",
        "The significant aspect is to import libraries for our use statistical analysis cases. Some of these include:\n",
        "\n",
        "- [Sklearn](https://scikit-learn.org/stable/)\n",
        "- [Matplotlib](https://matplotlib.org/)\n",
        "- [Seaborn](https://seaborn.pydata.org/)\n",
        "- [NLTK](https://www.nltk.org/)\n",
        "- wordcloud\n",
        "\n",
        "These libraries and frameworks are efficient in handling data which can be used for initial analysis. As we progress, we will be including more libraries.\n",
        "\n"
      ]
    },
    {
      "source": [
        "import nltk\n",
        "\n",
        "# Download the 'stopwords' dataset\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing,metrics,manifold\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,cross_val_predict\n",
        "from imblearn.over_sampling import ADASYN,SMOTE\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "import collections\n",
        "import matplotlib.patches as mpatches\n",
        "from sklearn.metrics import accuracy_score\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import xgboost\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "from sklearn.metrics import classification_report,roc_auc_score,roc_curve,r2_score,recall_score,confusion_matrix,precision_recall_curve\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold,KFold,StratifiedShuffleSplit\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "# Access the stop words after downloading\n",
        "stop_words = stopwords.words('english')\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA, TruncatedSVD,SparsePCA\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from plotly import tools\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sCTw5ATR5_ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "QJdQb8cr2iDA"
      },
      "cell_type": "code",
      "source": [
        "## Assess the shape of the data\n",
        "print(\"The Shape of the Dataset\".format(),train_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TzA8qDnk2iDB"
      },
      "cell_type": "markdown",
      "source": [
        "## Statistical Analysis-I\n",
        "\n",
        "This is the start of the analysis phase where we will first check the amount of data present in either of the sentiments.\n",
        "We will follow this up with some pictorial representations related to the words and frequency mappings.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "EqF8R_Ti2iDB"
      },
      "cell_type": "code",
      "source": [
        "good_reviews=train_df[train_df['sentiment']=='positive']['review']\n",
        "bad_reviews=train_df[train_df['sentiment']=='negative']['review']\n",
        "print(\"First 10 samples of good reviews\\n\".format(),good_reviews[:10])\n",
        "print(\"First 10 samples of bad reviews\\n\".format(),bad_reviews[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "KB1Uz9oX2iDB"
      },
      "cell_type": "code",
      "source": [
        "#Count of good and bad reviews\n",
        "count=train_df['sentiment'].value_counts()\n",
        "print('Total Counts of both sets'.format(),count)\n",
        "\n",
        "print(\"==============\")\n",
        "#Creating a function to plot the counts using matplotlib\n",
        "def plot_counts(count_good,count_bad):\n",
        "    plt.rcParams['figure.figsize']=(6,6)\n",
        "    plt.bar(0,count_good,width=0.6,label='Positive Reviews',color='Green')\n",
        "    plt.legend()\n",
        "    plt.bar(2,count_bad,width=0.6,label='Negative Reviews',color='Red')\n",
        "    plt.legend()\n",
        "    plt.ylabel('Count of Reviews')\n",
        "    plt.xlabel('Types of Reviews')\n",
        "    plt.show()\n",
        "\n",
        "count_good=train_df[train_df['sentiment']=='positive']\n",
        "count_bad=train_df[train_df['sentiment']=='negative']\n",
        "plot_counts(len(count_good),len(count_bad))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "G3aOz6Zs2iDB"
      },
      "cell_type": "code",
      "source": [
        "#Analyse the count of words in each segment- both positive and negative reviews\n",
        "#Function for checking word length\n",
        "def cal_len(data):\n",
        "    return len(data)\n",
        "\n",
        "#Create generic plotter with Seaborn\n",
        "def plot_count(count_ones,count_zeros,title_1,title_2,subtitle):\n",
        "    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n",
        "    sns.distplot(count_zeros,ax=ax1,color='Blue')\n",
        "    ax1.set_title(title_1)\n",
        "    sns.distplot(count_ones,ax=ax2,color='Red')\n",
        "    ax2.set_title(title_2)\n",
        "    fig.suptitle(subtitle)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "count_good_words=count_good['review'].str.split().apply(lambda z:cal_len(z))\n",
        "count_bad_words=count_bad['review'].str.split().apply(lambda z:cal_len(z))\n",
        "print(\"Positive Review Words:\" + str(count_good_words))\n",
        "print(\"Negative Review Words:\" + str(count_bad_words))\n",
        "plot_count(count_good_words,count_bad_words,\"Positive Review\",\"Negative Review\",\"Reviews Word Analysis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9sZgBli62iDC"
      },
      "cell_type": "code",
      "source": [
        "#Count Punctuations/Stopwords/Codes and other semantic datatypes\n",
        "#We will be using the \"generic_plotter\" function.\n",
        "\n",
        "count_good_punctuations=count_good['review'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))\n",
        "count_bad_punctuations=count_bad['review'].apply(lambda z:len([c for c in str(z) if c in string.punctuation]))\n",
        "plot_count(count_good_punctuations,count_bad_punctuations,\"Positive Review Punctuations\",\"Negative Review Punctuations\",\"Reviews Word Punctuation Analysis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Hrqp6Le92iDC"
      },
      "cell_type": "code",
      "source": [
        "#Analyse Stopwords\n",
        "\n",
        "def plot_count_1(count_ones,count_zeros,title_1,title_2,subtitle):\n",
        "    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n",
        "    sns.distplot(count_zeros,ax=ax1,color='Blue')\n",
        "    ax1.set_title(title_1)\n",
        "    sns.distplot(count_ones,ax=ax2,color='Orange')\n",
        "    ax2.set_title(title_2)\n",
        "    fig.suptitle(subtitle)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "stops=set(stopwords.words('english'))\n",
        "count_good_stops=count_good['review'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\n",
        "count_bad_stops=count_bad['review'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\n",
        "plot_count_1(count_good_stops,count_bad_stops,\"Positive Reviews Stopwords\",\"Negative Reviews Stopwords\",\"Reviews Stopwords Analysis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yk1i22TX2iDC"
      },
      "cell_type": "code",
      "source": [
        "## Checking number of Urls\n",
        "count_good_urls=count_good['review'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
        "count_bad_urls=count_bad['review'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
        "\n",
        "plot_count_1(count_good_stops,count_bad_stops,\"Positive Reviews URLs\",\"Negative Reviews URLs\",\"Reviews URLs Analysis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "mIWOFY1o2iDD"
      },
      "cell_type": "code",
      "source": [
        "#WordCloud Visualizations\n",
        "#Method for creating wordclouds\n",
        "from PIL import Image\n",
        "def display_cloud(data,img_path,color):\n",
        "    plt.subplots(figsize=(10,10))\n",
        "    mask = np.array(Image.open(img_path))\n",
        "    wc = WordCloud(stopwords=STOPWORDS,\n",
        "                   mask=mask, background_color=\"white\", contour_width=2, contour_color=color,\n",
        "                   max_words=2000, max_font_size=256,\n",
        "                   random_state=42, width=mask.shape[1],\n",
        "                   height=mask.shape[0])\n",
        "    wc.generate(' '.join(data))\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "display_cloud(train_df['review'],'../input/avenger-image-1/captain-america__silo.png','red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdOpf_XsWQMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "vegVc7_F2iDD"
      },
      "cell_type": "code",
      "source": [
        "#Wordlcouds for good reviews\n",
        "display_cloud( count_good['review'],'../input/avenger-image/avengers-endgame-imax-poster-crop.png','blue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gUO3RrnQ2iDD"
      },
      "cell_type": "code",
      "source": [
        "#Simplified counter function\n",
        "def create_corpus(word):\n",
        "    corpus=[]\n",
        "\n",
        "    for x in train_df[train_df['sentiment']==word]['review'].str.split():\n",
        "        for i in x:\n",
        "            corpus.append(i)\n",
        "    return corpus\n",
        "\n",
        "corpus=create_corpus('positive')\n",
        "counter=Counter(corpus)\n",
        "most=counter.most_common()\n",
        "x=[]\n",
        "y=[]\n",
        "for word,count in most[:100]:\n",
        "    if (word not in stops) :\n",
        "        x.append(word)\n",
        "        y.append(count)\n",
        "sns.barplot(x=y,y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xwJrkWyz2iDD"
      },
      "cell_type": "markdown",
      "source": [
        "## Inference From Analysis -I\n",
        "\n",
        "The following can be inferred from the data:\n",
        "\n",
        "- The dataset is balanced.\n",
        "- The dataset contains equal number of semantics for reviews of both polarity.\n",
        "- The dataset contains redundant words and html syntaxes.\n",
        "- Punctuations/stopwords are present in a equal distribution in the dataset.\n",
        "\n",
        "This tells us that we have to do lots of cleaning!\n",
        "\n",
        "\n",
        "<img src=\"https://i.pinimg.com/originals/c4/84/34/c484342c69562e5960fc2a8951d87c74.png\">\n"
      ]
    },
    {
      "metadata": {
        "id": "OZXupXQu2iDE"
      },
      "cell_type": "markdown",
      "source": [
        "## Statistical Analysis-II\n",
        "\n",
        "In this context , we will be exploring further into the analysis part. This would allow us to have a better idea which part of the data requires removal and which part can be transformed before applying any model on it.\n",
        "\n",
        "Here we will be looking into:\n",
        "\n",
        "- [Gram Statistics](https://albertauyeung.github.io/2018/06/03/generating-ngrams.html)\n",
        "\n",
        "Gram analysis is an essential tool which forms the base of preparing a common bag of words model containing relevant data. This process implies that we are taking into consideration which words are present in conjunction with other words with a maximum frequency in the dataset. Grams can be n-ary implying that we can have many gram analysis taking n-words together.For example: a Ternary Gram Analysis(Tri-gram) includes analysing sentences which have 3 words occuring together at a higher frequency.\n",
        "\n",
        "A detailed image of a ternary gram analysis using a famous example is provided:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/536/1*vZhxrBkCz-yN_rzZBqSKiA.png\">\n",
        "\n",
        "Another example is also provided:\n",
        "\n",
        "\n",
        "<img src=\"https://images.deepai.org/glossary-terms/867de904ba9b46869af29cead3194b6c/8ARA1.png\">\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "uxXAKsAY2iDE"
      },
      "cell_type": "code",
      "source": [
        "#Gram analysis on Training set- Bigram and Trigram\n",
        "stopword=set(stopwords.words('english'))\n",
        "def gram_analysis(data,gram):\n",
        "    tokens=[t for t in data.lower().split(\" \") if t!=\"\" if t not in stopword]\n",
        "    ngrams=zip(*[tokens[i:] for i in range(gram)])\n",
        "    final_tokens=[\" \".join(z) for z in ngrams]\n",
        "    return final_tokens\n",
        "\n",
        "\n",
        "#Create frequency grams for analysis\n",
        "\n",
        "def create_dict(data,grams):\n",
        "    freq_dict=defaultdict(int)\n",
        "    for sentence in data:\n",
        "        for tokens in gram_analysis(sentence,grams):\n",
        "            freq_dict[tokens]+=1\n",
        "    return freq_dict\n",
        "\n",
        "def horizontal_bar_chart(df, color):\n",
        "    trace = go.Bar(\n",
        "        y=df[\"n_gram_words\"].values[::-1],\n",
        "        x=df[\"n_gram_frequency\"].values[::-1],\n",
        "        showlegend=False,\n",
        "        orientation = 'h',\n",
        "        marker=dict(\n",
        "            color=color,\n",
        "        ),\n",
        "    )\n",
        "    return trace\n",
        "\n",
        "\n",
        "\n",
        "def create_new_df(freq_dict,):\n",
        "    freq_df=pd.DataFrame(sorted(freq_dict.items(),key=lambda z:z[1])[::-1])\n",
        "    freq_df.columns=['n_gram_words','n_gram_frequency']\n",
        "    #print(freq_df.head())\n",
        "    #plt.barh(freq_df['n_gram_words'][:20],freq_df['n_gram_frequency'][:20],linewidth=0.3)\n",
        "    #plt.show()\n",
        "    trace=horizontal_bar_chart(freq_df[:20],'orange')\n",
        "    return trace\n",
        "\n",
        "def plot_grams(trace_zero,trace_one):\n",
        "    fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n",
        "                          subplot_titles=[\"Frequent words of positive reviews\",\n",
        "                                          \"Frequent words of negative reviews\"])\n",
        "    fig.append_trace(trace_zero, 1, 1)\n",
        "    fig.append_trace(trace_ones, 1, 2)\n",
        "    fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n",
        "    py.iplot(fig, filename='word-plots')\n",
        "\n",
        "\n",
        "train_df_zero=count_bad['review']\n",
        "train_df_ones=count_good['review']\n",
        "\n",
        "print(\"Bi-gram analysis\")\n",
        "freq_train_df_zero=create_dict(train_df_zero[:200],2)\n",
        "#print(freq_train_df_zero)\n",
        "trace_zero=create_new_df(freq_train_df_zero)\n",
        "freq_train_df_ones=create_dict(train_df_ones[:200],2)\n",
        "#print(freq_train_df_zero)\n",
        "trace_ones=create_new_df(freq_train_df_ones)\n",
        "plot_grams(trace_zero,trace_ones)\n",
        "print(\"Tri-gram analysis\")\n",
        "freq_train_df_zero=create_dict(train_df_zero[:200],3)\n",
        "#print(freq_train_df_zero)\n",
        "trace_zero=create_new_df(freq_train_df_zero)\n",
        "freq_train_df_ones=create_dict(train_df_ones[:200],3)\n",
        "#print(freq_train_df_zero)\n",
        "trace_ones=create_new_df(freq_train_df_ones)\n",
        "plot_grams(trace_zero,trace_ones)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3ZGG-gnM2iDE"
      },
      "cell_type": "code",
      "source": [
        "#Lets fo a Penta-Gram analysis to get an idea how the vectorization will be performed\n",
        "print(\"Penta-gram analysis\")\n",
        "freq_train_df_zero=create_dict(train_df_zero[:200],5)\n",
        "#print(freq_train_df_zero)\n",
        "trace_zero=create_new_df(freq_train_df_zero)\n",
        "freq_train_df_ones=create_dict(train_df_ones[:200],5)\n",
        "#print(freq_train_df_zero)\n",
        "trace_ones=create_new_df(freq_train_df_ones)\n",
        "plot_grams(trace_zero,trace_ones)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9nXkJ2dH2iDE"
      },
      "cell_type": "markdown",
      "source": [
        "## Inference from Analysis - II\n",
        "\n",
        "In this section, we have analysed based on positional features of words in a corpus/sentence/paragraph. The Gram analysis,particularly the pentagram analysis provides an idea which sentences occur more often in the corpus. And in most of the cases, these bag of words are the ones picked up by any frequency vectorization technique.\n",
        "\n",
        "Thus this provides an outline as to the frequency of the conjuction of words which are occuring at the highest frequency. Another important aspect is that,there is a presence of certain html tags and punctuations which have to be removed as these are adding noise to the review corpus. This will be taken up in the cleaning phase."
      ]
    },
    {
      "metadata": {
        "id": "tojtK2lA2iDF"
      },
      "cell_type": "markdown",
      "source": [
        "## Time for some Cleaning!\n",
        "\n",
        "Before we move ahead , let us clean the dataset and remove the redundancies.This includes\n",
        "\n",
        "- HTML codes\n",
        "- URLs\n",
        "- Emojis\n",
        "- Stopwords\n",
        "- Punctuations\n",
        "- Expanding Abbreviations\n",
        "\n",
        "These will be sufficient for cleaning the corpus!\n",
        "\n",
        "[Regex](https://docs.python.org/3/howto/regex.html) is a very good tool which will help us to do this cleaning."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "LkheNkJT2iDF"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import re\n",
        "#Removes Punctuations\n",
        "def remove_punctuations(data):\n",
        "    punct_tag=re.compile(r'[^\\w\\s]')\n",
        "    data=punct_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "#Removes HTML syntaxes\n",
        "def remove_html(data):\n",
        "    html_tag=re.compile(r'<.*?>')\n",
        "    data=html_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "#Removes URL data\n",
        "def remove_url(data):\n",
        "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
        "    data=url_clean.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "#Removes Emojis\n",
        "def remove_emoji(data):\n",
        "    emoji_clean= re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    data=emoji_clean.sub(r'',data)\n",
        "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
        "    data=url_clean.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "train_df['review']=train_df['review'].apply(lambda z: remove_punctuations(z))\n",
        "\n",
        "train_df['review']=train_df['review'].apply(lambda z: remove_html(z))\n",
        "train_df['review']=train_df['review'].apply(lambda z: remove_url(z))\n",
        "train_df['review']=train_df['review'].apply(lambda z: remove_emoji(z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XjAXVkgg2iDF"
      },
      "cell_type": "code",
      "source": [
        "## In this case, we will be replacing some abbreviated pronouns with full forms (example:\"you've\"->you have\")\n",
        "def remove_abb(data):\n",
        "    data = re.sub(r\"he's\", \"he is\", data)\n",
        "    data = re.sub(r\"there's\", \"there is\", data)\n",
        "    data = re.sub(r\"We're\", \"We are\", data)\n",
        "    data = re.sub(r\"That's\", \"That is\", data)\n",
        "    data = re.sub(r\"won't\", \"will not\", data)\n",
        "    data = re.sub(r\"they're\", \"they are\", data)\n",
        "    data = re.sub(r\"Can't\", \"Cannot\", data)\n",
        "    data = re.sub(r\"wasn't\", \"was not\", data)\n",
        "    data = re.sub(r\"don\\x89Ûªt\", \"do not\", data)\n",
        "    data= re.sub(r\"aren't\", \"are not\", data)\n",
        "    data = re.sub(r\"isn't\", \"is not\", data)\n",
        "    data = re.sub(r\"What's\", \"What is\", data)\n",
        "    data = re.sub(r\"haven't\", \"have not\", data)\n",
        "    data = re.sub(r\"hasn't\", \"has not\", data)\n",
        "    data = re.sub(r\"There's\", \"There is\", data)\n",
        "    data = re.sub(r\"He's\", \"He is\", data)\n",
        "    data = re.sub(r\"It's\", \"It is\", data)\n",
        "    data = re.sub(r\"You're\", \"You are\", data)\n",
        "    data = re.sub(r\"I'M\", \"I am\", data)\n",
        "    data = re.sub(r\"shouldn't\", \"should not\", data)\n",
        "    data = re.sub(r\"wouldn't\", \"would not\", data)\n",
        "    data = re.sub(r\"i'm\", \"I am\", data)\n",
        "    data = re.sub(r\"I\\x89Ûªm\", \"I am\", data)\n",
        "    data = re.sub(r\"I'm\", \"I am\", data)\n",
        "    data = re.sub(r\"Isn't\", \"is not\", data)\n",
        "    data = re.sub(r\"Here's\", \"Here is\", data)\n",
        "    data = re.sub(r\"you've\", \"you have\", data)\n",
        "    data = re.sub(r\"you\\x89Ûªve\", \"you have\", data)\n",
        "    data = re.sub(r\"we're\", \"we are\", data)\n",
        "    data = re.sub(r\"what's\", \"what is\", data)\n",
        "    data = re.sub(r\"couldn't\", \"could not\", data)\n",
        "    data = re.sub(r\"we've\", \"we have\", data)\n",
        "    data = re.sub(r\"it\\x89Ûªs\", \"it is\", data)\n",
        "    data = re.sub(r\"doesn\\x89Ûªt\", \"does not\", data)\n",
        "    data = re.sub(r\"It\\x89Ûªs\", \"It is\", data)\n",
        "    data = re.sub(r\"Here\\x89Ûªs\", \"Here is\", data)\n",
        "    data = re.sub(r\"who's\", \"who is\", data)\n",
        "    data = re.sub(r\"I\\x89Ûªve\", \"I have\", data)\n",
        "    data = re.sub(r\"y'all\", \"you all\", data)\n",
        "    data = re.sub(r\"can\\x89Ûªt\", \"cannot\", data)\n",
        "    data = re.sub(r\"would've\", \"would have\", data)\n",
        "    data = re.sub(r\"it'll\", \"it will\", data)\n",
        "    data = re.sub(r\"we'll\", \"we will\", data)\n",
        "    data = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", data)\n",
        "    data = re.sub(r\"We've\", \"We have\", data)\n",
        "    data = re.sub(r\"he'll\", \"he will\", data)\n",
        "    data = re.sub(r\"Y'all\", \"You all\", data)\n",
        "    data = re.sub(r\"Weren't\", \"Were not\", data)\n",
        "    data = re.sub(r\"Didn't\", \"Did not\", data)\n",
        "    data = re.sub(r\"they'll\", \"they will\", data)\n",
        "    data = re.sub(r\"they'd\", \"they would\", data)\n",
        "    data = re.sub(r\"DON'T\", \"DO NOT\", data)\n",
        "    data = re.sub(r\"That\\x89Ûªs\", \"That is\", data)\n",
        "    data = re.sub(r\"they've\", \"they have\", data)\n",
        "    data = re.sub(r\"i'd\", \"I would\", data)\n",
        "    data = re.sub(r\"should've\", \"should have\", data)\n",
        "    data = re.sub(r\"You\\x89Ûªre\", \"You are\", data)\n",
        "    data = re.sub(r\"where's\", \"where is\", data)\n",
        "    data = re.sub(r\"Don\\x89Ûªt\", \"Do not\", data)\n",
        "    data = re.sub(r\"we'd\", \"we would\", data)\n",
        "    data = re.sub(r\"i'll\", \"I will\", data)\n",
        "    data = re.sub(r\"weren't\", \"were not\", data)\n",
        "    data = re.sub(r\"They're\", \"They are\", data)\n",
        "    data = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", data)\n",
        "    data = re.sub(r\"you\\x89Ûªll\", \"you will\", data)\n",
        "    data = re.sub(r\"I\\x89Ûªd\", \"I would\", data)\n",
        "    data = re.sub(r\"let's\", \"let us\", data)\n",
        "    data = re.sub(r\"it's\", \"it is\", data)\n",
        "    data = re.sub(r\"can't\", \"cannot\", data)\n",
        "    data = re.sub(r\"don't\", \"do not\", data)\n",
        "    data = re.sub(r\"you're\", \"you are\", data)\n",
        "    data = re.sub(r\"i've\", \"I have\", data)\n",
        "    data = re.sub(r\"that's\", \"that is\", data)\n",
        "    data = re.sub(r\"i'll\", \"I will\", data)\n",
        "    data = re.sub(r\"doesn't\", \"does not\",data)\n",
        "    data = re.sub(r\"i'd\", \"I would\", data)\n",
        "    data = re.sub(r\"didn't\", \"did not\", data)\n",
        "    data = re.sub(r\"ain't\", \"am not\", data)\n",
        "    data = re.sub(r\"you'll\", \"you will\", data)\n",
        "    data = re.sub(r\"I've\", \"I have\", data)\n",
        "    data = re.sub(r\"Don't\", \"do not\", data)\n",
        "    data = re.sub(r\"I'll\", \"I will\", data)\n",
        "    data = re.sub(r\"I'd\", \"I would\", data)\n",
        "    data = re.sub(r\"Let's\", \"Let us\", data)\n",
        "    data = re.sub(r\"you'd\", \"You would\", data)\n",
        "    data = re.sub(r\"It's\", \"It is\", data)\n",
        "    data = re.sub(r\"Ain't\", \"am not\", data)\n",
        "    data = re.sub(r\"Haven't\", \"Have not\", data)\n",
        "    data = re.sub(r\"Could've\", \"Could have\", data)\n",
        "    data = re.sub(r\"youve\", \"you have\", data)\n",
        "    data = re.sub(r\"donå«t\", \"do not\", data)\n",
        "train_df['review']=train_df['review'].apply(lambda z: remove_abb(z))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "qFNpI-gN2iDG"
      },
      "cell_type": "code",
      "source": [
        "train_df['review'][:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PsPrm7sc2iDG"
      },
      "cell_type": "markdown",
      "source": [
        "## The data is cleaned!\n",
        "\n",
        "Let us apply the Gram Statistics on the cleaned dataset!"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "CBYoINcD2iDG"
      },
      "cell_type": "code",
      "source": [
        "count_good=train_df[train_df['sentiment']=='positive']\n",
        "count_bad=train_df[train_df['sentiment']=='negative']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "TiSQu9mh2iDG"
      },
      "cell_type": "code",
      "source": [
        "#Apply Gram Analysis\n",
        "train_df_zero=count_bad['review']\n",
        "train_df_ones=count_good['review']\n",
        "print(\"Tri-gram analysis\")\n",
        "freq_train_df_zero=create_dict(train_df_zero[:200],3)\n",
        "#print(freq_train_df_zero)\n",
        "trace_zero=create_new_df(freq_train_df_zero)\n",
        "freq_train_df_ones=create_dict(train_df_ones[:200],3)\n",
        "#print(freq_train_df_zero)\n",
        "trace_ones=create_new_df(freq_train_df_ones)\n",
        "plot_grams(trace_zero,trace_ones)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "AYhYicu32iDH"
      },
      "cell_type": "code",
      "source": [
        "#Check with wordclouds again just to be sure!\n",
        "display_cloud(train_df['review'],'../input/avenger-image-1/captain-america__silo.png','blue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oYn60nyY2iDH"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning is completed!\n",
        "\n",
        "We have completed the cleaning step and gained significant insights about the corpus.\n",
        "\n",
        "<img src=\"https://filmschoolrejects.com/wp-content/uploads/2019/01/Spider-Man-Far-From-Home-700x500.png\">\n"
      ]
    },
    {
      "metadata": {
        "id": "P6rxQbJj2iDH"
      },
      "cell_type": "markdown",
      "source": [
        "## Transforming the Corpus!!\n",
        "\n",
        "Now at this stage the data is successfully cleaned and all redundant noises are removed. These steps are generic to any NLP pipeline which reduces the dimension of the data. Once the data is cleaned , we can again prune some words to their base form and reduce the sentence lengths. This is important because when we are applying any model (statistical, deep learning,  transformers,graphs), 2 different words from the same base word are encoded and tokenized in a different manner. For instance, the word \"watched\" and \"watching\" have the same root word \"watch\", however they are encoded separately with respect to any Tokenizer.\n",
        "\n",
        "\n",
        "To alleviate this issue, it is recommended to perform lemmatization on the text corpus so that the words can be reduced to their root semantic word. Morphological transformations such as \"watched\" and \"watching\", are converted to their base form through this method. Stemming , although can be used , is not recommended as it does not take into consideration the semantics of the sentence or the surrounding words which are present around it.Stemming also produces words which are not present in the vocabulary.\n",
        "\n",
        "For an in depth study of the same, please refer to the [Stanford documentation](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
        "\n",
        "For trying out stemming, you can use my porter stemmer [library](https://github.com/abhilash1910/Classic_Stemmer)\n",
        "\n",
        "But for now, we will be using NLTK for our lemmatization purposes. So [lets, get started!](http://www.nltk.org/api/nltk.stem.html?highlight=lemmatizer)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "35UYXls02iDH"
      },
      "cell_type": "code",
      "source": [
        "#Lemmatize the dataset\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "def lemma_traincorpus(data):\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    out_data=\"\"\n",
        "    for words in data:\n",
        "        out_data+= lemmatizer.lemmatize(words)\n",
        "    return out_data\n",
        "\n",
        "train_df['review']=train_df['review'].apply(lambda z: lemma_traincorpus(z))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "al2_Xx_C2iDI"
      },
      "cell_type": "code",
      "source": [
        "#check a sample from the lemmatized dataset\n",
        "train_df['review'][5:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "dFm3K2kz2iDI"
      },
      "cell_type": "code",
      "source": [
        "#For example let us try to stem them and check  a sample\n",
        "\n",
        "from nltk.stem import *\n",
        "def stem_traincorpus(data):\n",
        "    stemmer = PorterStemmer()\n",
        "    out_data=\"\"\n",
        "    for words in data:\n",
        "        out_data+= stemmer.stem(words)\n",
        "    return out_data\n",
        "\n",
        "sample_train_df=train_df[5:10]\n",
        "sample_train_df['review']=sample_train_df['review'].apply(lambda z: stem_traincorpus(z))\n",
        "sample_train_df['review']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ye05Tsl-2iDP"
      },
      "cell_type": "markdown",
      "source": [
        "## End of Dataset Preparation\n",
        "\n",
        "At this stage , we have covered the dataset preparation part of the pipeline. At this stage we have analysed the dataset , got an initial estimate about the words in the corpus. We performed cleaning, statistical analysis as well as lemmatization to prepare the dataset for EDA and successive steps."
      ]
    },
    {
      "metadata": {
        "id": "L5ahWUJF2iDP"
      },
      "cell_type": "markdown",
      "source": [
        "## Importance of Data Preprocessing and Cleaning\n",
        "\n",
        "The afore mentioned phase is one of the most important phase. If the textual data is not properly cleaned or processed, incorrect words/puncutations/urls and associated redundancies get added to the data. This impacts the performance when we will be creating static/dynamic embeddings and analysing the sentence/word vectors. In the context of embeddings,(and subsequently models), we will find that if we donot remove these inconsistencies, the vectors will not be properly placed. For example, if we apply a SOTA language embedding such as GPT-2 on unclean data containing such redundancies, the tokenizer will create separate tokens for them; which will lead the model to associate certain weights for these. These add to the redundancy and increase complexity of the model. When we will be extracting individual entries (word/sentence vectors),then these redundancies get added to the vector space and interfere with different metrics such as semantic similarity or classification/question answers etc.\n",
        "\n",
        "In real world, data is much more unclean as in most of the cases, data scientists work with unstructured data coming from MonogoDb or other databases. In some cases, the data may be scraped from websites which picks up a lot of inconsistencies- specially pdf, which when converted to textual format(using beautifulsoup library or similar),may contain some urls/tags.\n",
        "This paper provides a good [analysis](https://arxiv.org/abs/1808.00024).Cleaning is hence really important!\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3Q_kRUtt2iDP"
      },
      "cell_type": "markdown",
      "source": [
        "## Vectorization and Embeddings\n",
        "\n",
        "In this context, we will be vectorizing our dataset. This would allow us to convert our data to higher dimensional containers (matrices). These vectorization strategies allow the word corpus to be properly suitable for advanced semantic analysis.\n",
        "\n",
        "Here there are 2 variants of transforming the textual corpus to a numerical vector:\n",
        "\n",
        "- Vectorize without semantics\n",
        "- Retain Semantic Importance\n",
        "\n",
        "In the first case, vectorization strategy is used to provide a co-occurence probabilistic distribution for vectorization. Methods like TF-IDF,Count vectorization/One hot vectorization, falls under this criteria.These methods leverage statistical co-occurence probabilities and log likelihoods for determining the frequently occuring sentences or group of words in a corpus.\n",
        "\n",
        "The second case, relies on applying vectors with respect to semantic importance. Embeddings fall under this category. Embeddings are largely of 2 kinds\n",
        "\n",
        "- Static Embeddings: Word2Vec, Glove, Fasttext, Paragram\n",
        "- Dynamic Embeddings: ELMO, BERT & its variants, XLNet/Transformer-XL\n",
        "\n",
        "All of these embeddings rely on pretrained word vectors where a probabilistic score is attributed to each word in the corpus. These probabilities are plotted in a low dimensional plane and the \"meaning\" of the words are inferred from these vectors. Generally speaking cosine distance is taken as the major metric of similarity measurement between word and sentence vectors to infer similarity."
      ]
    },
    {
      "metadata": {
        "id": "l8v6TpJ42iDQ"
      },
      "cell_type": "markdown",
      "source": [
        "## Vectorization - TFIDF and Count\n",
        "\n",
        "We will move ahead with TFIDF and Count vectorization strategies and will be going in further sections.\n",
        "\n",
        "- [TF-IDF Vectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html): This works by applying a logarithmic term to inverse document frequency (IDF) part other than determining the \"TF\" or term freqency part. The formulation can be shown as follows:\n",
        "\n",
        "<img src=\"https://plumbr.io/app/uploads/2016/06/tf-idf.png\">\n",
        "\n",
        "- [Count Vectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html): This is a simpler vectorization technique which relies on frequency of occurence of a particular term in a document or corpus.\n",
        "\n",
        "A pictorial representation about the way in which vectorization occurs is provided:\n",
        "\n",
        "<img src=\"https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/assets/atap_0408.png\">\n",
        "\n",
        "Let us now go ahead and vectorize the corpus and test its dimensionality."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "WoYhVjG72iDQ"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tfidf_vect=TfidfVectorizer(stop_words='english',ngram_range=(1,3))\n",
        "train_tfidf=tfidf_vect.fit_transform(train_df['review'].values.tolist())\n",
        "train_tfidf.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HrXI639q2iDQ"
      },
      "cell_type": "code",
      "source": [
        "## Outputs from the TF-IDF transformed data\n",
        "print(train_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cHW6gqJO2iDR"
      },
      "cell_type": "markdown",
      "source": [
        "## Converting the labels to Binary Numerics\n",
        "\n",
        "Here we convert the labels into binary (1,0) values , which will be helpful when we apply tensor compression or dimensionality reduction algorithms for visualizing the vectors."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "O0g0rgix2iDR"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train_li=[]\n",
        "for i in range(len(train_df)):\n",
        "    if (train_df['sentiment'][i]=='positive'):\n",
        "        train_li.append(1)\n",
        "    else:\n",
        "        train_li.append(0)\n",
        "train_df['Binary']=train_li\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rsNswOSV2iDR"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualizing the  Vector Space\n",
        "\n",
        "As words and sentences are vectorized, the dimensions of the vector space becomes significantly large to be accomodated in a model. For any computation system it is recommended to keep the dimensions of a tensor (matrix) as small as possible and maintain its regularity.  For tensors with larger dimensions and irregular shapes, it is difficult for the system to perform any operation (matrix /tensor multiplication etc.). Complex operations like tensor differentiation (Jacobian) or numerical approximation is another difficult thing to do for large matrices. The rank plays an important aspect for these operations.\n",
        "\n",
        "Now, we have to reduce the dimensions ,else the kernel will run out of memory. For this  we wmploy 3 different decomposition techniques:\n",
        "\n",
        "- [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
        "- [SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)\n",
        "- [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n",
        "\n",
        "These algorithms rely on Eigen vector decomposition and Eigen matrices for creating smaller matrices. These reduced matrices are well-fitted to perform any numerical approximation tasks from differentiation to higher order non linear dynamics. [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) in general is a well known method and forms the base of all decomposition techniques.  Pictorially it operates as follows with the help of orthogonal Eigen vectors:\n",
        "\n",
        "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ica_vs_pca_thumb.png\">\n",
        "\n",
        "\n",
        "TSNE is a more sophisticated [method](https://lvdmaaten.github.io/publications/papers/AISTATS_2009.pdf)  which uses a non convex optimization along with gradient descent. This is different than Eigen Vector (convex optimization) method of PCA and hence different results may be obtained in different iterations. It is a memory intensive method and is often powerful at the expense of longer execution time.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/685/1*njEd7PiqBW-zW38E23Ho9w.png\">\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ChA8KqNt2iDR"
      },
      "cell_type": "code",
      "source": [
        "#Count Vectorization\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "def vectorize(data):\n",
        "    cv=CountVectorizer()\n",
        "    fit_data_cv=cv.fit_transform(data)\n",
        "    return fit_data_cv,cv\n",
        "\n",
        "#Tfidf vectorization from sklearn\n",
        "def tfidf(data):\n",
        "    tfidfv=TfidfVectorizer()\n",
        "    fit_data_tfidf=tfidfv.fit_transform(data)\n",
        "    return fit_data_cv,tfidfv\n",
        "\n",
        "def dimen_reduc_plot(test_data,test_label,option):\n",
        "    tsvd= TruncatedSVD(n_components=2,algorithm=\"randomized\",random_state=42)\n",
        "    tsne=TSNE(n_components=2,random_state=42) #not recommended instead use PCA\n",
        "    pca=SparsePCA(n_components=2,random_state=42)\n",
        "    if(option==1):\n",
        "        tsvd_result=tsvd.fit_transform(test_data)\n",
        "        plt.figure(figsize=(10,8))\n",
        "        colors=['orange','red']\n",
        "\n",
        "        sns.scatterplot(x=tsvd_result[:,0],y=tsvd_result[:,1],hue=test_label        )\n",
        "\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(10,10))\n",
        "        plt.scatter(tsvd_result[:,0],tsvd_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n",
        "        color_red=mpatches.Patch(color='red',label='Negative Review')\n",
        "        color_orange=mpatches.Patch(color='orange',label='Positive Review')\n",
        "        plt.legend(handles=[color_orange,color_red])\n",
        "        plt.title(\"TSVD\")\n",
        "        plt.show()\n",
        "    if(option==2):\n",
        "        tsne_result=tsne.fit_transform(test_data)\n",
        "        plt.figure(figsize=(10,8))\n",
        "        colors=['orange','red']\n",
        "        sns.scatterplot(x=tsne_result[:,0],y=tsne_result[:,1],hue=test_label)\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(10,10))\n",
        "        plt.scatter(x=tsne_result[:,0],y=tsne_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n",
        "        color_red=mpatches.Patch(color='red',label='Negative Review')\n",
        "        color_orange=mpatches.Patch(color='orange',label='Positive Review')\n",
        "        plt.legend(handles=[color_orange,color_red])\n",
        "        plt.title(\"PCA\")\n",
        "        plt.show()\n",
        "    if(option==3):\n",
        "        pca_result=pca.fit_transform(test_data.toarray())\n",
        "        plt.figure(figsize=(10,8))\n",
        "        colors=['orange','red']\n",
        "        sns.scatterplot(x=pca_result[:,0],y=pca_result[:,1],hue=test_label)\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(10,10))\n",
        "        plt.scatter(x=pca_result[:,0],y=pca_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n",
        "        color_red=mpatches.Patch(color='red',label='Negtive Review')\n",
        "        color_orange=mpatches.Patch(color='orange',label='Positive Review')\n",
        "        plt.legend(handles=[color_orange,color_red])\n",
        "        plt.title(\"TSNE\")\n",
        "        plt.show()\n",
        "train_data=train_df\n",
        "data_vect=train_data['review'].values\n",
        "data_vect_good=count_good['review'].values\n",
        "target_vect=train_data['Binary'].values\n",
        "target_data_vect_good=train_df[train_df['sentiment']=='positive']['Binary'].values\n",
        "data_vect_bad=count_bad['review'].values\n",
        "target_data_vect_bad=train_df[train_df['sentiment']=='positive']['Binary'].values\n",
        "train_data_cv,cv= vectorize(data_vect)\n",
        "real_review_train_data_cv,cv=vectorize(data_vect_good)\n",
        "\n",
        "print(train_data.head())\n",
        "dimen_reduc_plot(train_data_cv,target_vect,1)\n",
        "dimen_reduc_plot(real_review_train_data_cv,target_data_vect_good,1)\n",
        "dimen_reduc_plot(real_review_train_data_cv,target_data_vect_bad,1)\n",
        "# dimen_reduc_plot(train_data_cv,target_vect,3)\n",
        "# dimen_reduc_plot(real_review_train_data_cv,target_data_vect_good,3)\n",
        "# dimen_reduc_plot(train_data_cv,target_vect,2)\n",
        "# dimen_reduc_plot(real_review_train_data_cv,target_data_vect_good,2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BPxi59W92iDS"
      },
      "cell_type": "markdown",
      "source": [
        "## Why Reduction?\n",
        "\n",
        "Contextual word vectors are very large and they are determined with respect to the surrounding context.Visualizing large dimension tensors or vectors are computatinally very difficult. By lowering the dimension, the implication is that there is a tensor decomposition based on the above mentioned algorithms. Tensor decomposition is based on rank specifications of a tensor or matrix. Based on rank, these reductions rely on reducing the effective rank based on the number of components we would want to visualize. When we specify the number of components we would want the tensor to be compressed into, the effective rank reduction takes place by Eigen vector decomposition (which is a numerical method).\n",
        "\n",
        "The TSNE, being a gradient descent based algorithm, takes up a lot of computation space when tried with large amounts of data. This leads to exhaustive memory consumption.In this case, a better solution would be to batch process the data, before passing it to the TSNE compressor. Tensorflow provides [Projector](https://projector.tensorflow.org/) which is really helpful for visualizing different embedding vectors which we will be going into. A visualization of the same is provided:\n",
        "\n",
        "![image.png](attachment:image.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "sOrlKCub2iDS"
      },
      "cell_type": "markdown",
      "source": [
        "## Important Links for Reduction\n",
        "\n",
        "These are some important links for the reduction strategies:\n",
        "\n",
        "- [TSNE](https://www.datacamp.com/community/tutorials/introduction-t-sne)\n",
        "- [Dimension Reduction](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/#:~:text=Dimensionality%20reduction%20refers%20to%20techniques,input%20variables%20in%20a%20dataset.&text=Large%20numbers%20of%20input%20features,the%20number%20of%20input%20features.)\n",
        "- [PCA & TSNE-Good article](https://medium.com/analytics-vidhya/a-complete-guide-on-dimensionality-reduction-62d9698013d2)\n",
        "- [TFIDF and Reduction](https://towardsdatascience.com/2-latent-methods-for-dimension-reduction-and-topic-modeling-20ff6d7d547)\n",
        "- [Berkely Course Advanced](https://people.eecs.berkeley.edu/~jordan/courses/294-fall09/lectures/dimensionality/)\n",
        "- [SVD](https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/)\n",
        "- [U-V matrix decomposition- SVD](https://blog.statsbot.co/singular-value-decomposition-tutorial-52c695315254)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "k122GY8y2iDS"
      },
      "cell_type": "code",
      "source": [
        "#TSNE visualization on first 1000 samples\n",
        "train_data=train_df[:1000]\n",
        "data_vect=train_data['review'].values\n",
        "data_vect_good=count_good['review'].values\n",
        "target_vect=train_data['Binary'].values\n",
        "target_data_vect_good=train_df[train_df['sentiment']=='positive']['Binary'].values\n",
        "data_vect_bad=count_bad['review'].values\n",
        "target_data_vect_bad=train_df[train_df['sentiment']=='positive']['Binary'].values\n",
        "train_data_cv,cv= vectorize(data_vect)\n",
        "real_review_train_data_cv,cv=vectorize(data_vect_good)\n",
        "dimen_reduc_plot(train_data_cv,target_vect,3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WYts6pCU2iDT"
      },
      "cell_type": "markdown",
      "source": [
        "## Convert Input DataFrame to a List\n",
        "\n",
        "This phase is helpful if we would like to investigate individual word embeddings or sentence embeddings. Differentiating the individual rows of text makes it easier to pass into static and dynamic embedding models."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3DKoVvRO2iDT"
      },
      "cell_type": "code",
      "source": [
        "check_df=list(train_df['review'].str.split())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i2YSEj_d2iDT"
      },
      "cell_type": "markdown",
      "source": [
        "## Semantic Embeddings\n",
        "\n",
        "In this context, we will be looking into semantic embeddings. These include embeddings which can either by static and dynamic. Word Embeddings fall under this category.\n",
        "\n",
        "Word Embeddings: These are vector space transformations of the words present in the corpus. When converted to vectors, several metrics can be applied like finding similarity, distance measurement between the vectors, numerical transforms of the vectors. With word vectors, we can specify semantic similarity between different words or collection of words. A pictorial representation of word vectors compressed with Dimension reduction methods is [provided below](https://www.tensorflow.org/tutorials/text/word_embeddings):\n",
        "\n",
        "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding.jpg?raw=1\">\n",
        "\n",
        "\n",
        "In this scenario, we will be focussing on all embedding algorithms. Primarily we be starting with Word2Vec and will be understanding advanced BERT/GPT architectures."
      ]
    },
    {
      "metadata": {
        "id": "Kf70ykE-2iDT"
      },
      "cell_type": "markdown",
      "source": [
        "## Static Word Embeddings\n",
        "\n",
        "These embeddings are pre-trained on large corpuses like Wikipedia, News corpuses.etc. However, the base of these algorithms rely on 2 important techniques:\n",
        "\n",
        "- [Skipgram](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1580/0*xqhh7Gd64VAQ1Sny.png\">\n",
        "\n",
        "\n",
        "- [Common Bag of Words Model](https://analyticsindiamag.com/the-continuous-bag-of-words-cbow-model-in-nlp-hands-on-implementation-with-codes/)\n",
        "\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/800/1*UVe8b6CWYykcxbBOR6uCfg.png\">\n",
        "\n",
        "\n",
        "This blog by [Lilian](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html) is a great introduction.\n",
        "\n",
        "The Skipgram approach tries to predict contextual words and phrases given a single word, whereas the CBOW approach tries to predict a single correct word given the context.\n",
        "\n",
        "[Jay's blog](http://jalammar.github.io/mit-analytics-lab-talk/) is a great resource for more learning!\n"
      ]
    },
    {
      "metadata": {
        "id": "FBVzFWDt2iDU"
      },
      "cell_type": "markdown",
      "source": [
        "## Word2Vec and its variants\n",
        "\n",
        "[Word2Vec](https://arxiv.org/abs/1301.3781) is one of the traditional algorithms which was emphasized based on Heirarchical Softmax as well as with simplistic RNNs. [Gensim](https://radimrehurek.com/gensim/models/word2vec.html) provides a great way to use and start with Word2Vec.\n",
        "The Word2Vec algorithm builds by using the Skipgram model as well as the Common Bag of Words Model. Both the models are described in the links.\n",
        "\n",
        "Some references:\n",
        "\n",
        "- [Nathan's blog](https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/)\n",
        "- [Illya's paper](https://arxiv.org/abs/1310.4546)\n",
        "- [Jason's Blog](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/)\n",
        "- [Advanced Resource](https://adventuresinmachinelearning.com/word2vec-keras-tutorial/)\n",
        "\n",
        "An overview of hte sipgram process:\n",
        "\n",
        "<img src=\"https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/skip-gram-architecture.png\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Huffman Algorithm forms the base of Hierarchical softmax based [Word2vec algorithm](https://leimao.github.io/article/Hierarchical-Softmax/). This uses the huffman coding algorithm  for compression.\n",
        "\n",
        "<img src=\"https://miro.medium.com/proxy/1*a4idodtq60y2U5HqpB_MTQ.png\">\n",
        "\n",
        "[This resource](https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281) is also helpful."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "otA-ZYCo2iDU"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "## Load word2vec algorithm from gensim\n",
        "from gensim.models import Word2Vec,KeyedVectors\n",
        "\n",
        "model=Word2Vec(check_df,min_count=1)\n",
        "word_li=list(model.wv.vocab)\n",
        "print(word_li[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7MDxkC5q2iDU"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize the word Vector\n",
        "\n",
        "In this case, we will be visualizing the word \"reviewers\" vector projection as well as its corresponding tensor."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Z_w5Vo9e2iDU"
      },
      "cell_type": "code",
      "source": [
        "#View the Tensor\n",
        "print(model)\n",
        "print(model['reviewers'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ssA9HtGg2iDV"
      },
      "cell_type": "code",
      "source": [
        "#View the Embedding Word Vector\n",
        "plt.plot(model['reviewers'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "pwvoOrcG2iDV"
      },
      "cell_type": "code",
      "source": [
        "##save the modeled words produced from Word2Vec\n",
        "model.save('word2vec_model.bin')\n",
        "loaded_model=KeyedVectors.load('word2vec_model.bin')\n",
        "print(loaded_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U3IM7Crj2iDV"
      },
      "cell_type": "markdown",
      "source": [
        "## Dimension reduction of the embedding vectors\n",
        "\n",
        "Let us try to visualize the compressed and decomposed embedding space based on first 50 entries in the dataset.Once we plot using the Word2Vec Skipgram/CBOW model, we can visualize the relative positioning of the words close to each other. The famous example using \"kings\",\"queens\" is provided below:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/327/1*keqyBCQ5FL6A7DZLrXamvQ.png\">\n",
        "\n",
        "Cosine Distance measurement is one metric which determines closeness of the 2 word vectors.When we plot vectors using matplotlib, we will be seeing graphs like ECG(electro-cardiograms) diagrams. The reason being each word is analysed on the basis of the number of words in its surrounding words.\n",
        "\n",
        "![image.png](attachment:image.png)\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "EQ2R01vQ2iDV"
      },
      "cell_type": "code",
      "source": [
        "#Measure Cosine distance\n",
        "distance=model.similarity('reviewers','injustice')\n",
        "print(distance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xtiUoeYG2iDW"
      },
      "cell_type": "code",
      "source": [
        "# PCA transform in 2D for visualization of embedded words\n",
        "from matplotlib import pyplot\n",
        "pca = PCA(n_components=2)\n",
        "transformation_model=loaded_model[loaded_model.wv.vocab]\n",
        "result = pca.fit_transform(transformation_model[:50])\n",
        "# create a scatter plot of the projection\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "words = list(loaded_model.wv.vocab)\n",
        "for i, word in enumerate(words[:50]):\n",
        "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6vJkhdiw2iDW"
      },
      "cell_type": "markdown",
      "source": [
        "## Variants of Word2Vec algorithms\n",
        "\n",
        "These variants include :\n",
        "\n",
        "- [FastText](https://github.com/facebookresearch/fastText)\n",
        "- [Glove](https://nlp.stanford.edu/projects/glove/)\n",
        "- [Google News Vectors](https://code.google.com/archive/p/word2vec/)\n",
        "\n",
        "Some important links:\n",
        "\n",
        "- [FastText Models](https://fasttext.cc/docs/en/english-vectors.html)\n",
        "- [Resource](https://www.analyticsvidhya.com/blog/2017/07/word-representations-text-classification-using-fasttext-nlp-facebook/)\n",
        "- [Glove Source](https://github.com/stanfordnlp/GloVe)\n",
        "\n",
        "\n",
        "All of these are traditional static embeddings . However they are very powerful on their own terms. These can be used with any neural network or classifier model with correct activations to suit our purpose. Word2Vec is the first SOTA model which relies on Softmax and probabilistic log likelihood of softmax to generate the predicted outputs. Word2vec has laid the foundation of all the algorithms which we saw here as well as variants in Node2Vec, Doc2Vec etc.\n",
        "\n",
        "\n",
        "An interactive view of these vector embeddings can be seen below:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XRTcVPVo2iDW"
      },
      "cell_type": "code",
      "source": [
        "#Using Google News Embeddings For our corpus\n",
        "google_news_embed='../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'\n",
        "google_loaded_model=KeyedVectors.load_word2vec_format(google_news_embed,binary=True)\n",
        "print(google_loaded_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "UQhiRBWB2iDX"
      },
      "cell_type": "code",
      "source": [
        "#Visualize the Word Vectors\n",
        "plt.plot(google_loaded_model['reviews'])\n",
        "plt.plot(google_loaded_model['injustice'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "BsMafaPA2iDX"
      },
      "cell_type": "code",
      "source": [
        "# PCA transform in 2D for visualization of google news embedded words\n",
        "from matplotlib import pyplot\n",
        "pca = PCA(n_components=2)\n",
        "transformation_model=google_loaded_model[google_loaded_model.wv.vocab]\n",
        "result = pca.fit_transform(transformation_model[:50])\n",
        "# create a scatter plot of the projection\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "words = list(google_loaded_model.wv.vocab)\n",
        "for i, word in enumerate(words[:50]):\n",
        "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7fpdlQ_Z2iDX"
      },
      "cell_type": "markdown",
      "source": [
        "## Working with Glove Embeddings\n",
        "\n",
        "Glove embeddings rely on global vector representations mechanism, which is an unsupervised algorithm. This captures both the global corpus statistics as well as local semantic information. Glove vectors used here can be converted from \"txt\" format to Word2Vec format by using scripts provided in the Gensim library.This allows us to manipulate the glove embeddings in a manner similar to Word2Vec and apply the similarity metric.The loss function for the glove relies on logistic regression of the log co-occurence probabilities.\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ugLKz4_u2iDX"
      },
      "cell_type": "code",
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "glove_file='../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt'\n",
        "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
        "glove_loaded=glove2word2vec(glove_file, word2vec_output_file)\n",
        "print(glove_loaded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "g-boPZPS2iDY"
      },
      "cell_type": "code",
      "source": [
        "glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
        "plt.plot(glove_model['reviews'])\n",
        "plt.plot(glove_model['injustice'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "hXS6rkXn2iDY"
      },
      "cell_type": "code",
      "source": [
        "# PCA transform in 2D for visualization of glove embedded words\n",
        "from matplotlib import pyplot\n",
        "pca = PCA(n_components=2)\n",
        "transformation_model=glove_model[glove_model.wv.vocab]\n",
        "result = pca.fit_transform(transformation_model[:50])\n",
        "# create a scatter plot of the projection\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "words = list(glove_model.wv.vocab)\n",
        "for i, word in enumerate(words[:50]):\n",
        "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9Jfs0NuC2iDY"
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec,KeyedVectors\n",
        "\n",
        "fasttext_file=\"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\n",
        "print(fasttext_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "EuVWkVyG2iDY"
      },
      "cell_type": "code",
      "source": [
        "fasttext_model = KeyedVectors.load_word2vec_format(fasttext_file, binary=False)\n",
        "plt.plot(fasttext_model['reviews'])\n",
        "plt.plot(fasttext_model['injustice'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "bY3HqH0M2iDZ"
      },
      "cell_type": "code",
      "source": [
        "# PCA transform in 2D for visualization of glove embedded words\n",
        "from matplotlib import pyplot\n",
        "pca = PCA(n_components=2)\n",
        "transformation_model=fasttext_model[fasttext_model.wv.vocab]\n",
        "result = pca.fit_transform(transformation_model[:50])\n",
        "# create a scatter plot of the projection\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "words = list(fasttext_model.wv.vocab)\n",
        "for i, word in enumerate(words[:50]):\n",
        "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4kn3Y3lP2iDZ"
      },
      "cell_type": "markdown",
      "source": [
        "## Model agnostic procedure to create Embedding Matrix\n",
        "\n",
        "From the embeddings discussed above, all of these can be used to create Embedding Matrix . The importance of building a Embedding matrix is to have mutual co-occurence embedding probabilities (vectors) of all the words present in the corpus. This is the format in which neural network frameworks like Keras, Tensorflow uses. These embedding vectors are then passed through deep learning layers. The flexibility of these static embeddings is that ,they provide a good benchmark on the initial task (classification) in our case.These also provide a very good approximation to the amount of percentage accuracy or loss which can be achieved with a coupled Encoder-Decoder/Transformer like architectures.\n",
        "\n",
        "In this case, we will be using the Keras/Tensorflow framework to build the matrix. Though we will not be building neural networks yet, this provides an insight as to how to build the Embedding Layer of a Keras Neural Network architecture.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/688/1*zR61FG9RUd6ul4ecXA_euQ.jpeg\">\n",
        "\n",
        "\n",
        "Some tutorials for starting with NLP in KEras:\n",
        "\n",
        "- [Resource](https://keras.io/examples/nlp/)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "d4O-sFX72iDZ"
      },
      "cell_type": "code",
      "source": [
        "#Creating Embedding Matrix\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "maxlen=1000\n",
        "max_features=5000\n",
        "embed_size=300\n",
        "\n",
        "train_sample=train_df['review']\n",
        "\n",
        "#Tokenizing steps- must be remembered\n",
        "tokenizer=Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(train_sample))\n",
        "train_sample=tokenizer.texts_to_sequences(train_sample)\n",
        "\n",
        "#Pad the sequence- To allow same length for all vectorized words\n",
        "train_sample=pad_sequences(train_sample,maxlen=maxlen)\n",
        "\n",
        "\n",
        "\n",
        "EMBEDDING_FILE = '../input/wikinews300d1msubwordvec/wiki-news-300d-1M-subword.vec'\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
        "\n",
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "plt.plot(embedding_matrix[20])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FjexuG3n2iDa"
      },
      "cell_type": "markdown",
      "source": [
        "## End of Static Embeddings\n",
        "\n",
        "Thus we saw the power of traditional static embeddings and also the way to create embedding matrices from those word vectors. The Word2Vec and its variant algorithms are hence very powerful for  initial benchmarking for any classification tasks. These old SOTA models are hence very useful for us in NLP.\n",
        "\n",
        "<img src=\"https://ftw.usatoday.com/wp-content/uploads/sites/90/2019/04/captain-america.jpg\">\n"
      ]
    },
    {
      "metadata": {
        "id": "qtVsE2eq2iDa"
      },
      "cell_type": "markdown",
      "source": [
        "## Dynamic Embeddings\n",
        "\n",
        "Deep contextual embeddings and sentence/word vectors falls under dynamic embeddings. These embeddings are current SOTA and these are deep contextual embeddings ,implying that there is a need for robust Neural NEtwork models for these architectures.\n",
        "\n",
        "Since we will not be going into depth about the model architectures of each of these, we will be brushing over the concetps required for creating these embedding models. The following models lie within the scope of these embeddings:\n",
        "\n",
        "- [ELMO](https://arxiv.org/abs/1802.05365)\n",
        "- [Transformers](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "Both these papers are essentially important for their contributions to contextual deep embeddings."
      ]
    },
    {
      "metadata": {
        "id": "MtC71CRh2iDa"
      },
      "cell_type": "markdown",
      "source": [
        "## ELMO - Brief Overview\n",
        "\n",
        "ELMO is a contextualised deep embedding model, which is dynamic and semi supervised.\n",
        "Word representations are functions of entire input sequence,and are computed on top of 2 bidirectional LSTM with character convolutions. The standard architecture for ELMO is as follows:\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/Bert-language-modeling.png\">\n",
        "\n",
        "The most important aspect is tasks specific combinations of intermediate layer representations of the Bilstm which allows retention of the words/long sentence sequences in the embedding space .\n",
        "\n",
        "[Jay's blog provides a good walkthrough](http://jalammar.github.io/illustrated-bert/).\n",
        "\n",
        "Bidirectional transfer learning (BiLSTM architecture) is important in this aspect.\n",
        "\n",
        "<img src=\"http://jalammar.github.io/images/elmo-forward-backward-language-model-embedding.png\">\n"
      ]
    },
    {
      "metadata": {
        "id": "td16oP1j2iDa"
      },
      "cell_type": "markdown",
      "source": [
        "## Using ELMO Embeddings\n",
        "\n",
        "Traditionally, Elmo embeddings could be used from [tensorflow hub](https://tfhub.dev/). The original implementation can be found in [AllenNLP](https://allennlp.org/elmo). Some resources for using ELMO :\n",
        "\n",
        "- [TFHub](https://tfhub.dev/google/elmo/1)\n",
        "- [ELMO-Good article](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/)\n",
        "- [ELMO-article](https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604)\n"
      ]
    },
    {
      "metadata": {
        "id": "tGbdbSR62iDb"
      },
      "cell_type": "markdown",
      "source": [
        "## Restrictions with Tensorflow TF 2.0\n",
        "\n",
        "\n",
        "Previously there used to be no issues, working with ELMO from tf hub with tensorflow version <2.0. With TF versions morethan 2.0, there are some issues with loading the embeddings (as eager execution during graph computation fails). Hence it is recommended to use ELMO-2 with Tensorflow <2.0 (favourably 1.15).\n",
        "\n",
        "ELMO-3 can be used with Tensorflow 2.0"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "id": "kmDQYola2iDb"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.3.1\n",
        "!pip install \"tensorflow_hub>=0.6.0\"\n",
        "!pip3 install tensorflow_text==1.15\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "bzz9tHEF2iDb"
      },
      "cell_type": "code",
      "source": [
        "#Convert the textual reviews to list for analysing sentences(sentence vectors)\n",
        "z=train_df['review'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "52eBTf5g2iDb"
      },
      "cell_type": "code",
      "source": [
        "##Tensorflow Hub ELMO-2\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "\n",
        "elmo = hub.load(\"https://tfhub.dev/google/elmo/2\")\n",
        "\n",
        "def create_elmo_embeddings(data):\n",
        "    embed=elmo(data,signature=\"default\",as_dict=True)[\"elmo\"]\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run(tf.tables_initializer())\n",
        "        out_x=sess.run(embed)\n",
        "        #out_y=ses.run(tf.reduce_mean(embed,1))\n",
        "        return out_x\n",
        "elmo_input=z[:2]\n",
        "elmo_output=create_elmo_embeddings(elmo_input)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aj6gUsJB2iDc"
      },
      "cell_type": "markdown",
      "source": [
        "## Transformers\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://static01.nyt.com/images/2007/07/02/arts/Trans1600.jpg?quality=75&auto=webp&disable=upscale\">\n",
        "\n",
        "\n",
        "We come to Transformer Embeddings  for which the most important aspect is the Transformer architecture. Since we will be diving in depth into architectures in the Machine LEarning Training session (model building), it is safe to have a glimpse of a traditional Transformer Architecture.\n",
        "\n",
        "<img src=\"https://i0.wp.com/esciencegroup.com/wp-content/uploads/2020/02/01.png?resize=506%2C641&ssl=1\">\n",
        "\n",
        "We will be working with the [HuggingFace](https://huggingface.co/) repository as it contains all SOTA Transformer models. In this context, it is useful to mention some important resources:\n",
        "\n",
        "- [Transformer Keras](https://keras.io/examples/nlp/text_classification_with_transformer/)\n",
        "- [Kaggle Kernel](https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb)\n",
        "\n",
        "\n",
        "However in this case, since we would be using the models just for extracting embeddings or features, it is important to know the intermediate layers which should be chosen. Since Transformer architectures are really huge, (BERT/GPT variants), it is very complicated to fully understand which layer should be extracted for the features. While BERT, the first Transformer, relies on 2 tokens ([CLS] and [SEP]) ,extracting the sentence embedding vectors are done after extracting the last output layer. However , different models have different number of layers, and in this case, we will exploring a model agnostic way to extract sentence embeddings and performing similarity check with all of the models.\n",
        "\n",
        "\n",
        "It is recommended to follow this [article](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/) before going further\n",
        "\n",
        "<img src=\"http://jalammar.github.io/images/bert-next-sentence-prediction.png\">"
      ]
    },
    {
      "metadata": {
        "id": "B7FpN3Td2iDc"
      },
      "cell_type": "markdown",
      "source": [
        "## BERT Embeddings\n",
        "\n",
        "[BERT](https://arxiv.org/abs/1810.04805) is a traditional SOTA transformer architecture published by Google Research which uses bidirectional pretraining . The importance of using BERT is that it has 2 important aspects:\n",
        "\n",
        "- Msked Language Model (MLM)\n",
        "- Next Sentence Prediction(NSP)\n",
        "\n",
        "The bidirectional pre-training is essentially helpful to be used for any tasks. The [Huggingface](https://miro.medium.com/max/876/0*ViwaI3Vvbnd-CJSQ.png) implementation is helpful for fine-tuning BERT for any language modelling task. The BERT architecture falls under an encoder-decoder(Transformer) model as follows:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/876/0*ViwaI3Vvbnd-CJSQ.png\">\n",
        "\n",
        "\n",
        "For fine-tuning and pre-training for different downstream tasks like Q/A, Classification, Language Modelling, Multiple Choice, NER etc. different layers of the BERT are used.\n",
        "\n",
        "<img src=\"https://d3i71xaburhd42.cloudfront.net/df2b0e26d0599ce3e70df8a9da02e51594e0e992/15-Figure4-1.png\">\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "e8nBbN5t2iDc"
      },
      "cell_type": "markdown",
      "source": [
        "## Finetuning BERT for Embeddings\n",
        "\n",
        "For finetuning, it is to be kept in mind, there are many ways to do this. We are using BERT from Huggingface repository while it can also be used from [TF-HUB](https://tfhub.dev/s?module-type=text-embedding) or from [Google-Research repository](https://github.com/google-research/bert). The reason for using HuggingFace is that the same codebase is applicable for all language models. The 3 most important input features that any language model asks for is:\n",
        "\n",
        "- input_ids\n",
        "- attention_masks\n",
        "- token_ids\n",
        "\n",
        "Let us first try to analyse and understand how BERT  tokenizers, and model can be used in this context. The [BERT](https://huggingface.co/transformers/model_doc/bert.html) documentation provides an outline of how to use BERT tokenizers and also modify it for downstream tasks.\n",
        "\n",
        "Generally by virtue of transfer learning through weight transfer, we use pretrained [BERT models](https://huggingface.co/transformers/pretrained_models.html) from the list. This allows us to finetune it to extract only the embeddings. Since we are using Keras, we have to build up a small model containing an Input Layer and apply the tokenized(encoded) input ids, attention masks as input to the pretrained and loaded BERT model.This is very similar to creating a very own classification model for BERT using Keras/Tensorflow, but since we will be needing only the Embeddings it is safe to extract only the sentence vectors in the last layer of the model output. In most of the cases , we will see that the dimensions of the output vector is (x,768) where x depends on the number of tokenized input features. For this we extract the [CLS] tokenized feature from the ouput to just extract the sentence embeddings.\n",
        "\n",
        "<img src=\"http://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\">\n",
        "\n",
        "\n",
        "Some important resources which may be helpful:\n",
        "\n",
        "- [Blog](https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a)\n",
        "- [Extensive Nice Blog](https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b)\n",
        "- [Good Kernel](https://www.kaggle.com/shirishsharma/nlp-from-embeddings-and-rnns-to-bert)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7LSDwjXV2iDc"
      },
      "cell_type": "code",
      "source": [
        "#tokenize and encode the inputs\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizer,TFBertModel\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)\n",
        "bert_model = transformers.TFBertModel.from_pretrained('bert-large-uncased')\n",
        "def bert_encode(data,maximum_length) :\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        encoded = tokenizer.encode_plus(\n",
        "\n",
        "          data[i],\n",
        "          add_special_tokens=True,\n",
        "          max_length=maximum_length,\n",
        "          pad_to_max_length=True,\n",
        "\n",
        "          return_attention_mask=True,\n",
        "\n",
        "        )\n",
        "\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "    return np.array(input_ids),np.array(attention_masks)\n",
        "\n",
        "train_input_ids,train_attention_masks = bert_encode(train_df['review'][:5],1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Fr5nDMJ-2iDd"
      },
      "cell_type": "code",
      "source": [
        "#Visualize the attention masks and input ids.\n",
        "train_attention_masks,train_input_ids\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "6uz8_LAy2iDd"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Build a miniature model for extracting the embeddings\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras import layers\n",
        "input_ids = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
        "input_masks_ids = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32')\n",
        "bert_output=bert_model([input_ids,input_masks_ids])[0]\n",
        "bert_output.shape\n",
        "bert_output[:,0,:]\n",
        "model=Model(inputs=[input_ids,input_masks_ids],outputs=[bert_output])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kM794RFd2iDd"
      },
      "cell_type": "markdown",
      "source": [
        "## Realize the Size of the BERT Model\n",
        "\n",
        "The size of the simple NN model built with BERT as intermediate Embedding Layer can be observed. Bert-large-uncased has 24-layer, 1024-hidden, 16-heads, 336M parameters and trained on lower-cased English text."
      ]
    },
    {
      "metadata": {
        "id": "nNACMsfD2iDe"
      },
      "cell_type": "markdown",
      "source": [
        "## Using the Transfomer Method\n",
        "\n",
        "\n",
        "In this case , we will be using the HuggingFace Transformer method for extracting sentence embeddings. This is a rather simpler method as we only need to extract the last layer from from the model output. The model in this case is bert-base-uncased (12-layer, 768-hidden, 12-heads, 110M parameters.Trained on lower-cased English text.).This code segment is model agnostic and can be used for any variats of BERT (except T5, GPT variants).\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "BVGzhlTJ2iDe"
      },
      "cell_type": "code",
      "source": [
        "#Use the tokenizer and model  from the Transformers and determine the output features from the last hidden layer.\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "def get_embeddings(model_name,tokenizer,name,inp):\n",
        "    tokenizer = tokenizer.from_pretrained(name)\n",
        "    model = model_name.from_pretrained(name)\n",
        "    input_ids = tf.constant(tokenizer.encode(inp))[None, :]  # Batch size 1\n",
        "    outputs = model(input_ids)\n",
        "    last_hidden_states = outputs[0]\n",
        "    cls_token=last_hidden_states[0]\n",
        "    return cls_token\n",
        "cls_token=get_embeddings(TFBertModel,BertTokenizer,'bert-base-uncased',z[0])\n",
        "cls_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Nh8D_6Ns2iDe"
      },
      "cell_type": "code",
      "source": [
        "# For visualizing the embeddings\n",
        "print(cls_token.shape)\n",
        "plt.plot(cls_token[0])\n",
        "plt.plot(cls_token[1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_IJ3w6Oa2iDf"
      },
      "cell_type": "markdown",
      "source": [
        "## Alternate Strategy With Transformers-One For All\n",
        "\n",
        "Sentence Vectors can be determined with the help of [Pipeline in Transformers](https://huggingface.co/transformers/main_classes/pipelines.html). This is a robust and efficient way to generate sentence vectors and compute corrspoinding distances between those vectors. It is a faster way which applies to almost all transformers.\n",
        "\n",
        "<img src=\"https://www.sideshow.com/wp/wp-content/uploads/2019/05/InfinityStones-Infographic-01.jpg\">"
      ]
    },
    {
      "metadata": {
        "id": "hyd_GkS62iDg"
      },
      "cell_type": "markdown",
      "source": [
        "## DistilBERT\n",
        "\n",
        "[This](https://huggingface.co/transformers/model_doc/distilbert.html) is a distilled version of pretraining BERT to produce a lightweight version of it. It is analogous to teacher supervision of a neural network learning to optimize tis weights. [DistilBERT Paper](https://arxiv.org/abs/1910.01108) provides an insight why it is 40% smaller but preserves 95% of BERT's weights for transfer learning.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/groundai-web-prod/media%2Fusers%2Fuser_14%2Fproject_391208%2Fimages%2FKD_figures%2Ftransformer_distillation.png\">\n",
        "\n",
        "\n",
        "A very neat representation on the model workflow is provided here:\n",
        "\n",
        "<img src=\"http://jalammar.github.io/images/distilBERT/bert-input-to-output-tensor-recap.png\">"
      ]
    },
    {
      "metadata": {
        "id": "2r7_Ead82iDg"
      },
      "cell_type": "markdown",
      "source": [
        "## The Cosine Distance Metric\n",
        "\n",
        "\n",
        "In this context, we will be using Cosine similarity metric from [Scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html). But we can use them after we extract the last hidden layer from the model output(similar to BERT)."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GOm5k0Ia2iDg"
      },
      "cell_type": "code",
      "source": [
        "#Distil BERT Embeddings\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, pipeline, TFDistilBertModel\n",
        "from scipy.spatial.distance import cosine\n",
        "def transformer_embedding(name,inp,model_name):\n",
        "\n",
        "    model = model_name.from_pretrained(name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "    pipe = pipeline('feature-extraction', model=model,\n",
        "                tokenizer=tokenizer)\n",
        "    features = pipe(inp)\n",
        "    features = np.squeeze(features)\n",
        "    return features\n",
        "embedding_features1=transformer_embedding('distilbert-base-uncased',z[0],TFDistilBertModel)\n",
        "embedding_features2=transformer_embedding('distilbert-base-uncased',z[1],TFDistilBertModel)\n",
        "distance=1-cosine(embedding_features1[0],embedding_features2[0])\n",
        "print(distance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "sfrOWUbO2iDg"
      },
      "cell_type": "code",
      "source": [
        "#Visualize embeddings\n",
        "plt.plot(embedding_features1[0])\n",
        "plt.plot(embedding_features2[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pSRLTIeO2iDh"
      },
      "cell_type": "markdown",
      "source": [
        "## BERT Embeddings with Alternate Strategy\n",
        "\n",
        "Here we see the new methodology applied for BERT using the Pipeline module of Trasnformers."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "IgaPQcaa2iDh"
      },
      "cell_type": "code",
      "source": [
        "#BERT embeddings\n",
        "from transformers import AutoTokenizer, pipeline, TFBertModel\n",
        "bert_features1=transformer_embedding('bert-base-uncased',z[0],TFBertModel)\n",
        "bert_features2=transformer_embedding('bert-base-uncased',z[1],TFBertModel)\n",
        "distance=1-cosine(bert_features1[0],bert_features2[0])\n",
        "print(distance)\n",
        "plt.plot(bert_features1[0])\n",
        "plt.plot(bert_features2[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sl8ZcNJy2iDh"
      },
      "cell_type": "markdown",
      "source": [
        "## Roberta Model\n",
        "\n",
        "[Roberta Model](https://huggingface.co/transformers/model_doc/roberta.html) is a robust and large model built by [Facebook Research](https://github.com/pytorch/fairseq/tree/master/examples/roberta), to alleviate undertrained nature of BERT. It trains in much larger mini-batch sizes. [This](https://cloud.google.com/tpu/docs/tutorials/roberta-pytorch) provides a good model of how to train Roberta on Google cloud.The original paper can be found [here](https://arxiv.org/abs/1907.11692), and the model architecture is provided.\n",
        "\n",
        "<img src=\"https://camo.githubusercontent.com/f5c0d05eb0635cdd0e17e137265af23fa825b1d4/68747470733a2f2f646c2e666261697075626c696366696c65732e636f6d2f584c4d2f786c6d5f6669677572652e6a7067\">\n",
        "\n",
        "\n",
        "Resources:\n",
        "\n",
        "- [Blog](https://medium.com/towards-artificial-intelligence/a-robustly-optimized-bert-pretraining-approach-f6b6e537e6a6)\n",
        "- [Blog-2](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "54OXAUwt2iDh"
      },
      "cell_type": "code",
      "source": [
        "##Roberta Embeddings\n",
        "from transformers import AutoTokenizer, pipeline, TFRobertaModel\n",
        "roberta_features1=transformer_embedding('roberta-base',z[0],TFRobertaModel)\n",
        "roberta_features2=transformer_embedding('roberta-base',z[1],TFRobertaModel)\n",
        "distance=1-cosine(roberta_features1[0],roberta_features2[0])\n",
        "print(distance)\n",
        "plt.plot(roberta_features1[0])\n",
        "plt.plot(roberta_features2[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T8opaSjE2iDi"
      },
      "cell_type": "markdown",
      "source": [
        "## XLNet Embeddings\n",
        "\n",
        "This [paper](https://arxiv.org/abs/1906.08237) provides an important outline of the modifications made on top of BERT for producing XLNet. It applies an autoregressive language model and has the 2 most important points:\n",
        "\n",
        "- Enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order\n",
        "- Overcomes the limitations of BERT thanks to its autoregressive formulation.\n",
        "\n",
        "It is a permutation language model and a pictorial representation can be :\n",
        "\n",
        "<img src=\"https://zdnet2.cbsistatic.com/hub/i/r/2019/06/21/2a4e6548-9dee-491d-b638-8cfae9bbb2fe/resize/1200x900/ab279544c2631111754a357ada50ef29/google-xlnet-architecture-2019.png\">\n",
        "\n",
        "\n",
        "Resources:\n",
        "- [Blog](https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/)\n",
        "- [Xlnet](https://www.borealisai.com/en/blog/understanding-xlnet/)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Jhet6nsr2iDi"
      },
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, pipeline, TFXLNetModel\n",
        "xlnet_features1=transformer_embedding('xlnet-base-cased',z[0],TFXLNetModel)\n",
        "xlnet_features2=transformer_embedding('xlnet-base-cased',z[1],TFXLNetModel)\n",
        "distance=1-cosine(xlnet_features1[0],xlnet_features2[0])\n",
        "print(distance)\n",
        "plt.plot(xlnet_features1[0])\n",
        "plt.plot(xlnet_features2[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rI80Jo9U2iDi"
      },
      "cell_type": "markdown",
      "source": [
        "## BART Model\n",
        "\n",
        "[This](https://arxiv.org/abs/1910.13461) is alternate SOTA model to denoise sentence2 sentence pretraining for natural language generation,comprehension etc. The most important points can be summarized as:\n",
        "\n",
        "\n",
        "- Bart uses a standard seq2seq/machine translation architecture with a bidirectional encoder (like BERT) and a left-to-right decoder (like GPT).\n",
        "\n",
        "- The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme, where spans of text are replaced with a single mask token.\n",
        "\n",
        "- BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n",
        "\n",
        "The architecture contains these encoder -decoder modules :\n",
        "<img src=\"https://miro.medium.com/max/3138/1*Qss9gtS1nw_sgcG1pMAM2A.png\">\n",
        "\n",
        "<img src=\"https://miseciara.files.wordpress.com/2013/11/bart.gif\">\n",
        "\n",
        "\n",
        "Some resources:\n",
        "- [Blog](https://medium.com/dair-ai/bart-are-all-pretraining-techniques-created-equal-e869a490042e)\n",
        "- [Blog-BART](https://medium.com/analytics-vidhya/revealing-bart-a-denoising-objective-for-pretraining-c6e8f8009564)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tlUEfTEq2iDi"
      },
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, pipeline, BartModel\n",
        "bart_features1=transformer_embedding('facebook/bart-base',z[0],BartModel)\n",
        "bart_features2=transformer_embedding('facebook/bart-base',z[1],BartModel)\n",
        "distance=1-cosine(bart_features1[0],bart_features2[0])\n",
        "print(distance)\n",
        "plt.plot(bart_features1[0])\n",
        "plt.plot(bart_features2[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zwu2JCHz2iDj"
      },
      "cell_type": "markdown",
      "source": [
        "## Albert Model\n",
        "\n",
        "[This](https://arxiv.org/abs/1909.11942) is a lighter version of BERT which splits the embedding matrix into 2 smaller matrices and uses repeated splitting in between the transformer layers.Some important points:\n",
        "\n",
        "\n",
        "- ALBERT is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n",
        "\n",
        "- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same number of (repeating) layers.\n",
        "\n",
        "Resources:\n",
        "-[Source code](https://github.com/google-research/ALBERT)\n",
        "- [Blog](https://medium.com/@lessw/meet-albert-a-new-lite-bert-from-google-toyota-with-state-of-the-art-nlp-performance-and-18x-df8f7b58fa28)\n",
        "- [Blog](https://medium.com/doxastar/from-bert-to-albert-pre-trained-langaug-models-5865aa5c3762)\n",
        "\n",
        "<img src=\"data:image/jpeg;base64,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\">"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "N38T6Bz_2iDk"
      },
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, pipeline, TFAlbertModel\n",
        "albert_features1=transformer_embedding('albert-base-v1',z[0],TFAlbertModel)\n",
        "albert_features2=transformer_embedding('albert-base-v1',z[1],TFAlbertModel)\n",
        "distance=1-cosine(albert_features1[0],albert_features2[0])\n",
        "print(distance)\n",
        "plt.plot(albert_features1[0])\n",
        "plt.plot(albert_features2[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "uIpSwpRO2iDk"
      },
      "cell_type": "code",
      "source": [
        "#sophisticated variants of BERT\n",
        "from transformers import AutoTokenizer, pipeline, FlaubertModel\n",
        "flaubert_features1=transformer_embedding('flaubert/flaubert_base_cased',z[0],FlaubertModel)\n",
        "flaubert_features2=transformer_embedding('flaubert/flaubert_base_cased',z[1],FlaubertModel)\n",
        "distance=1-cosine(flaubert_features1[0],flaubert_features2[0])\n",
        "print(distance)\n",
        "plt.plot(flaubert_features1[0])\n",
        "plt.plot(flaubert_features2[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q1IbSpSs2iDk"
      },
      "cell_type": "markdown",
      "source": [
        "## GPT-Generative Pretraining\n",
        "\n",
        "[This](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) is a different model from BERT and its variants built primarily for NLG (generative modelling).  GPT has the following important points:\n",
        "\n",
        "\n",
        "- GPT is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n",
        "\n",
        "- GPT was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n",
        "\n",
        "Some resources are helpful:\n",
        "\n",
        "- [GPT](https://medium.com/dataseries/openai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4)\n",
        "- [Openai](https://openai.com/blog/better-language-models/)\n",
        "- [Imgae GPT](https://openai.com/blog/image-gpt/)\n",
        "\n",
        "<img src=\"https://www.topbots.com/wp-content/uploads/2019/04/cover_GPT_web.jpg\">"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "PbOfSL5-2iDk"
      },
      "cell_type": "code",
      "source": [
        "#GPT embeddings\n",
        "from transformers import AutoTokenizer, pipeline, TFOpenAIGPTModel\n",
        "def transformer_gpt_embedding(name,inp,model_name):\n",
        "\n",
        "    model = model_name.from_pretrained(name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "    tokenizer.pad_token = \"[PAD]\"\n",
        "    pipe = pipeline('feature-extraction', model=model,\n",
        "                tokenizer=tokenizer)\n",
        "    features = pipe(inp)\n",
        "    features = np.squeeze(features)\n",
        "    return features\n",
        "gpt_features1=transformer_gpt_embedding('openai-gpt',z[0],TFOpenAIGPTModel)\n",
        "gpt_features2=transformer_gpt_embedding('openai-gpt',z[1],TFOpenAIGPTModel)\n",
        "distance=1-cosine(gpt_features1[0],gpt_features2[0])\n",
        "print(distance)\n",
        "plt.plot(gpt_features1[0])\n",
        "plt.plot(gpt_features2[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EwukE3Ku2iDl"
      },
      "cell_type": "markdown",
      "source": [
        "## GPT-2\n",
        "\n",
        "<img src=\"http://jalammar.github.io/images/gpt2/openAI-GPT-2-3.png\">\n",
        "\n",
        "It is a [robust model](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.Some important aspects:\n",
        "\n",
        "- GPT-2 is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n",
        "\n",
        "- GPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n",
        "\n",
        "Resources:\n",
        "\n",
        "- [GPT-2](http://jalammar.github.io/illustrated-gpt2/)\n",
        "- [Source Code](https://github.com/openai/gpt-2)\n",
        "- [Blog](https://www.analyticsvidhya.com/blog/2019/07/openai-gpt2-text-generator-python/)\n",
        "- [Blog](https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8)\n",
        "\n",
        "It is important to note the effect of Attention and masking in GPT-2 model. These are represented in the diagram:\n",
        "\n",
        "<img src=\"http://jalammar.github.io/images/gpt2/gpt2-self-attention-qkv-1-2.png\">\n",
        "<img src=\"http://jalammar.github.io/images/gpt2/gpt2-self-attention-qkv-3-2.png\">\n",
        "\n",
        "Self Attention:\n",
        "\n",
        "<img src=\"http://jalammar.github.io/images/gpt2/gpt2-self-attention-split-attention-heads-1.png\">\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "umcgQLWW2iDm"
      },
      "cell_type": "code",
      "source": [
        "#GPT-2\n",
        "from transformers import AutoTokenizer, pipeline, TFGPT2Model\n",
        "\n",
        "gpt2_features1=transformer_gpt_embedding('openai-gpt',z[0],TFGPT2Model)\n",
        "gpt2_features2=transformer_gpt_embedding('openai-gpt',z[1],TFGPT2Model)\n",
        "distance=1-cosine(gpt2_features1[0],gpt2_features2[0])\n",
        "print(distance)\n",
        "plt.plot(gpt2_features1[0])\n",
        "plt.plot(gpt2_features2[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "0J0EtCoY2iDm"
      },
      "cell_type": "code",
      "source": [
        "#Electra\n",
        "\n",
        "from transformers import AutoTokenizer, pipeline, TFElectraModel\n",
        "electra_features1=transformer_embedding('google/electra-small-discriminator',z[0],TFElectraModel)\n",
        "electra_features2=transformer_embedding('google/electra-small-discriminator',z[1],TFElectraModel)\n",
        "distance=1-cosine(electra_features1[0],electra_features2[0])\n",
        "print(distance)\n",
        "plt.plot(electra_features1[0])\n",
        "plt.plot(electra_features2[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "B439LU6D2iDm"
      },
      "cell_type": "code",
      "source": [
        "#Longformer\n",
        "from transformers import AutoTokenizer, pipeline, TFLongformerModel\n",
        "longformer_features1=transformer_embedding('allenai/longformer-base-4096',z[0],TFLongformerModel)\n",
        "longformer_features2=transformer_embedding('allenai/longformer-base-4096',z[1],TFLongformerModel)\n",
        "distance=1-cosine(longformer_features1[0],longformer_features2[0])\n",
        "print(distance)\n",
        "plt.plot(longformer_features1[0])\n",
        "plt.plot(longformer_features2[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tBczaEv_2iDm"
      },
      "cell_type": "markdown",
      "source": [
        "## BERT Embeddings From Pytorch\n",
        "\n",
        "Since we have explored Transformer Embeddings, we will be looking into using pretrained BERT embedding from [Pytorch](https://pytorch.org/hub/huggingface_pytorch-transformers/).\n",
        "We will be downloading BERT from [Pytorch Pretrained BERT](https://pypi.org/project/pytorch-pretrained-bert/).\n",
        "This is one of the initial starting libraries for Huggingface.\n",
        "\n",
        "\n",
        "Some important resources:\n",
        "\n",
        "- [Notebook](https://www.kaggle.com/christofhenkel/bert-embeddings-lstm)\n",
        "- [Blog](https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1)\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/875/1*Pvyx8-QWdo_WBghNOd-zGA.png\">"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HxxFgtYD2iDn"
      },
      "cell_type": "code",
      "source": [
        "#Import BERT and the variables\n",
        "import os\n",
        "BERT_MODEL = 'bert-base-uncased'\n",
        "CASED = 'uncased' in BERT_MODEL\n",
        "INPUT = '../input/'\n",
        "TEXT_COL = 'comment_text'\n",
        "MAXLEN = 250\n",
        "os.system('pip install --no-index --find-links=\"../input/pytorchpretrainedbert/\" pytorch_pretrained_bert')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "REMelQY-2iDn"
      },
      "cell_type": "code",
      "source": [
        "#Error Cause\n",
        "os.system('pip install --no-index --find-links=\"../input/pytorchpretrainedbert/\" pytorch_pretrained_bert')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "UU8_R59W2iDn"
      },
      "cell_type": "code",
      "source": [
        "#Load  from pytorch pretrained model- weights\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "\n",
        "#BERT_FP = '../input/torch-bert-weights/bert-base-uncased/bert-base-uncased/'\n",
        "#Function for creating BERT embeddings-matrix\n",
        "def bert_embedding_matrix():\n",
        "    bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "    print(bert)\n",
        "    bert_embeddings = list(bert.children())[0]\n",
        "    bert_word_embeddings = list(bert_embeddings.children())[0]\n",
        "    mat = bert_word_embeddings.weight.data.numpy()\n",
        "    return mat\n",
        "embedding_matrix = bert_embedding_matrix()\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GMJyrfMN2iDn"
      },
      "cell_type": "code",
      "source": [
        "plt.plot(embedding_matrix[0])\n",
        "plt.plot(embedding_matrix[1])\n",
        "plt.plot(embedding_matrix[2])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qmbldmd_2iDo"
      },
      "cell_type": "markdown",
      "source": [
        "## Embeddings Conclusion and Key Takeaways\n",
        "\n",
        "This Notebook has provided an idea of using a code segment for multiple Transformer based embedding models . For this it is to be kept in mind that most of these embedding models are very big architectures-Transformers - which we will be taking up in the next session on Model building. Since the codes are mostly in Tensorflow Keras, some additional resources are provided for running them in Pytorch:\n",
        "\n",
        "- [BERT Embeddings Pytorch](https://www.kaggle.com/abhilash1910/bertsimilarity-library)\n",
        "- [NLP Workshop](https://www.kaggle.com/abhilash1910/nlp-workshop-2-ml-india)\n",
        "- [Workshop1](https://www.kaggle.com/abhilash1910/nlp-workshop-ml-india)\n",
        "\n",
        "Will be updating with more reseources. We have explored embeddings (static and dynamic) and also had an idea on the entire NLP pipeline till embeddings!! Now we know the power of model agnostic code  in NLP.\n",
        "\n",
        "<img src=\"https://img.cinemablend.com/filter:scale/quill/2/0/d/8/1/6/20d81656d25ffbc06d6b2e382241661c629972e1.jpg?mw=600\">\n",
        "\n",
        "\n",
        "Next module- [Notebook](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop-2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "NLP(End to end): CLL NLP workshop",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}