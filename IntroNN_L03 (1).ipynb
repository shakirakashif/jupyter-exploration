{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install matplotlib\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter\n",
    "import io\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This lab is designed to introduce you to the basics of deep learning by interacting with a pre-built model. You'll understand the workflow of a deep learning project, including data preprocessing, model architecture, and making predictions. The goal is to familiarize yourself with the basics of deep learning without writing any code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the VGG16 model\n",
    "model = VGG16(weights='imagenet')\n",
    "\n",
    "# Display the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess an image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    # Load the image\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "\n",
    "    # Convert the image to a numpy array\n",
    "    img_array = img_to_array(img)\n",
    "\n",
    "    # Expand dimensions to fit the model input\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Preprocess the image\n",
    "    img_array = preprocess_input(img_array)\n",
    "\n",
    "    return img, img_array\n",
    "\n",
    "# Load and preprocess a sample image\n",
    "sample_image, processed_image = load_and_preprocess_image('/path/to/your/images/Ziggy_jeans.jpg')\n",
    "\n",
    "# Display the sample image\n",
    "plt.imshow(sample_image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Making Predictions\n",
    "\n",
    "In this step, you will learn how to use a pre-trained VGG16 model to make predictions on new data â€” specifically, images that you upload. This process involves several stages, including uploading your image, preprocessing it to meet the model's input requirements, and finally, using the model to predict what the image represents.\n",
    "\n",
    "### 4.1. Uploading an Image\n",
    "\n",
    "To make predictions, the model first needs an input image. This is where the interactive widget comes into play:\n",
    "\n",
    "- **Upload Button**: You will see an upload button labeled 'Upload Image'. Clicking this button will open your file explorer.\n",
    "- **Selecting Image**: Navigate through your files and select the image you want to analyze. The accepted formats usually include JPEG, PNG, and others, depending on the configuration.\n",
    "- **Uploading**: After selecting the image, it will be uploaded and stored in a temporary location in this notebook's environment for processing.\n",
    "\n",
    "### 4.2. Image Preprocessing\n",
    "\n",
    "The uploaded image needs to be preprocessed to ensure compatibility with the VGG16 model. This preprocessing involves several steps:\n",
    "\n",
    "1. **Reading the Image**: The uploaded image is read into the notebook as a data stream and then converted into a format that the model can process (typically, a numerical array).\n",
    "2. **Resizing**: VGG16 requires input images to be of a specific size (224x224 pixels). If your uploaded image is of a different size, it will be resized.\n",
    "3. **Color Channels Adjustment**: The model expects images to have three color channels (Red, Green, Blue). If the uploaded image has a different format (like grayscale), it will be adjusted accordingly.\n",
    "4. **Array Conversion and Dimension Expansion**: The image is then converted into a multi-dimensional array (a format that the neural network can process), and dimensions are added or expanded to match the input shape that the model expects.\n",
    "5. **Normalization**: Finally, pixel values are normalized or scaled to a range that the model is trained on, usually [0, 1] or [-1, 1], depending on the specific preprocessing requirements of the VGG16 model.\n",
    "\n",
    "After these preprocessing steps, your image is ready to be fed into the model for prediction.\n",
    "\n",
    "In the next section, you'll learn how the model uses this preprocessed image to predict the content of the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(processed_image)\n",
    "\n",
    "# Decode and print the predictions\n",
    "decoded_predictions = decode_predictions(predictions, top=3)[0]\n",
    "print(decoded_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Understanding Predictions\n",
    "\n",
    "After making predictions in the previous step, you now have a chance to dive deeper into the model's behavior. In this step, you will explore how slight modifications to the input image (like rotation, adding noise, or changing brightness) can affect the model's predictions. This process helps in understanding the robustness and sensitivity of the neural network.\n",
    "\n",
    "### 5.1. Observing Model Predictions\n",
    "\n",
    "Initially, you will observe the predictions made by the model on the uploaded image. The model provides a set of predictions, each with a corresponding probability score indicating the model's confidence in its prediction. For example:\n",
    "\n",
    "- **Prediction 1**: 'Labrador Retriever' with a confidence of 94.5%\n",
    "- **Prediction 2**: 'Golden Retriever' with a confidence of 5.0%\n",
    "- **Prediction 3**: 'Beagle' with a confidence of 0.5%\n",
    "\n",
    "These predictions give you a glimpse into what the model 'sees' in the image.\n",
    "\n",
    "### 5.2. Experimenting with Image Modifications\n",
    "\n",
    "Understanding a model's predictions goes beyond just seeing what it predicts; it's also about understanding how certain changes to the input can alter these predictions. This section involves interacting with different image modifications:\n",
    "\n",
    "1. **Rotation**: Rotate the image by a certain degree. Observe how rotating the image affects the predictions. Does the model still recognize the object correctly?\n",
    "2. **Adding Noise**: Introduce random visual noise to the image. This can help you understand how noise-tolerant the model is.\n",
    "3. **Brightness Adjustment**: Change the brightness of the image. This helps in understanding if the model's predictions are sensitive to changes in lighting conditions.\n",
    "\n",
    "For each modification, you will:\n",
    "\n",
    "- Apply the modification to the uploaded image.\n",
    "- Use the model to make new predictions on the modified image.\n",
    "- Observe how the predictions change in response to the modifications.\n",
    "\n",
    "### 5.3. Reflection and Analysis\n",
    "\n",
    "After observing how the model's predictions change with different image modifications, reflect on the following questions:\n",
    "\n",
    "- How robust is the model to changes in the input image?\n",
    "- Which types of modifications affect the model's predictions the most?\n",
    "- What does this tell you about the features that the model is focusing on when making predictions?\n",
    "\n",
    "This step is crucial in understanding not just what a model sees, but also how stable and reliable its 'vision' is under various conditions. It encourages a deeper understanding of the model's capabilities and limitations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload button to load images\n",
    "upload = widgets.FileUpload()\n",
    "display(upload)\n",
    "\n",
    "# Button to make predictions\n",
    "predict_button = widgets.Button(description=\"Make Prediction\")\n",
    "display(predict_button)\n",
    "\n",
    "# Function to handle button click\n",
    "def on_click(change):\n",
    "    img_data = list(upload.value.values())[0]['content']\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "    img = img.resize((224, 224))\n",
    "    \n",
    "    # Preprocess and predict\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    predictions = model.predict(img_array)\n",
    "    decoded_predictions = decode_predictions(predictions, top=3)[0]\n",
    "    \n",
    "    # Display predictions\n",
    "    print(decoded_predictions)\n",
    "\n",
    "predict_button.on_click(on_click)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Discussion\n",
    "Reflect on the lab activities. Discuss how the pre-trained model was able to make predictions, the role of data preprocessing, and the impact of input modifications on the model's predictions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
